{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Timesketch Timesketch is an open-source tool for collaborative forensic timeline analysis. Using sketches you and your collaborators can easily organize your timelines and analyze them all at the same time. Add meaning to your raw data with rich annotations, comments, tags and stars. Obligatory Fine Print : This is not an official Google product (experimental or otherwise), it is just code that happens to be owned by Google.","title":"Home"},{"location":"#timesketch","text":"Timesketch is an open-source tool for collaborative forensic timeline analysis. Using sketches you and your collaborators can easily organize your timelines and analyze them all at the same time. Add meaning to your raw data with rich annotations, comments, tags and stars. Obligatory Fine Print : This is not an official Google product (experimental or otherwise), it is just code that happens to be owned by Google.","title":"Timesketch"},{"location":"community/2021_timesketch_summit/","text":"Timesketch Summit 2021 On March 10th 2021 16:00 - 21:00 UTC the first Timesketch summit will take place. About Timesketch Timesketch is an open source tool for collaborative forensic timeline analysis. Using sketches you and your collaborators can easily organize your timelines and analyze them all at the same time. Add meaning to your raw data with rich annotations, comments, tags and stars. Where The Timesketch summit will take place virtually. Who can participate Everyone who is interested in Timesketch is welcome. The event is free . Registration Closed Agenda and talks Presentations: 16:00 - 18:40 UTC Workshops: 19:00 - 21:00 UTC All times are in UTC 16:00 - 16:10 Welcome & Logistics 16:10 - 16:20 Timesketch history Johan Berggren 16:20 - 16:50 Timesketch Basics introduction Session ( Bart Inglot ) 16:50 - 17:00 Q&A Slot 1 17:00 - 17:30 Using Timesketch in real life. ( CSIS Security Group ) 17:30 - 17:40 Q&A Slot 2 17:40 - 18:15 Lightning talk(s): Jupyter Notebooks by Kristinn Gu\u00f0j\u00f3nsson Timesketch analyzers by Johan Berggren DFTimewolf by Thomas Chopitea Sigma in Timesketch by Alexander J\u00e4ger Timesketch used with Velociraptor Eric Capuano Whitney Champion 18:15 - 18:30 Timesketch Roadmap & the future Johan Berggren 18:30 - 18:35 Wrap up first block and outline Workshops (Alexander J\u00e4ger) Break 19:00 - 19:05 Workshop explanation / group forming Alexander J\u00e4ger 19:05 - 21:00 Workshops (might be shorter depending on engagement, questions etc): Colab / API usage (Szechuan sauce) by Kristinn Gu\u00f0j\u00f3nsson Timesketch entry UX with Szechuan sauce by Alexander J\u00e4ger Call for Presentations & Workshops Closed Questions If you have any questions, please reach out to the team via slack in the Timesketch channel","title":"2021 Summit"},{"location":"community/2021_timesketch_summit/#timesketch-summit-2021","text":"On March 10th 2021 16:00 - 21:00 UTC the first Timesketch summit will take place.","title":"Timesketch Summit 2021"},{"location":"community/2021_timesketch_summit/#about-timesketch","text":"Timesketch is an open source tool for collaborative forensic timeline analysis. Using sketches you and your collaborators can easily organize your timelines and analyze them all at the same time. Add meaning to your raw data with rich annotations, comments, tags and stars.","title":"About Timesketch"},{"location":"community/2021_timesketch_summit/#where","text":"The Timesketch summit will take place virtually.","title":"Where"},{"location":"community/2021_timesketch_summit/#who-can-participate","text":"Everyone who is interested in Timesketch is welcome. The event is free .","title":"Who can participate"},{"location":"community/2021_timesketch_summit/#registration","text":"Closed","title":"Registration"},{"location":"community/2021_timesketch_summit/#agenda-and-talks","text":"Presentations: 16:00 - 18:40 UTC Workshops: 19:00 - 21:00 UTC All times are in UTC 16:00 - 16:10 Welcome & Logistics 16:10 - 16:20 Timesketch history Johan Berggren 16:20 - 16:50 Timesketch Basics introduction Session ( Bart Inglot ) 16:50 - 17:00 Q&A Slot 1 17:00 - 17:30 Using Timesketch in real life. ( CSIS Security Group ) 17:30 - 17:40 Q&A Slot 2 17:40 - 18:15 Lightning talk(s): Jupyter Notebooks by Kristinn Gu\u00f0j\u00f3nsson Timesketch analyzers by Johan Berggren DFTimewolf by Thomas Chopitea Sigma in Timesketch by Alexander J\u00e4ger Timesketch used with Velociraptor Eric Capuano Whitney Champion 18:15 - 18:30 Timesketch Roadmap & the future Johan Berggren 18:30 - 18:35 Wrap up first block and outline Workshops (Alexander J\u00e4ger) Break 19:00 - 19:05 Workshop explanation / group forming Alexander J\u00e4ger 19:05 - 21:00 Workshops (might be shorter depending on engagement, questions etc): Colab / API usage (Szechuan sauce) by Kristinn Gu\u00f0j\u00f3nsson Timesketch entry UX with Szechuan sauce by Alexander J\u00e4ger","title":"Agenda and talks"},{"location":"community/2021_timesketch_summit/#call-for-presentations-workshops","text":"Closed","title":"Call for Presentations &amp; Workshops"},{"location":"community/2021_timesketch_summit/#questions","text":"If you have any questions, please reach out to the team via slack in the Timesketch channel","title":"Questions"},{"location":"community/resources/","text":"Mailing lists User list: https://groups.google.com/forum/#!forum/timesketch-users Developer list: https://groups.google.com/forum/#!forum/timesketch-dev Slack community Join the DFIR Timesketch Slack community .","title":"Resources"},{"location":"community/resources/#mailing-lists","text":"User list: https://groups.google.com/forum/#!forum/timesketch-users Developer list: https://groups.google.com/forum/#!forum/timesketch-dev","title":"Mailing lists"},{"location":"community/resources/#slack-community","text":"Join the DFIR Timesketch Slack community .","title":"Slack community"},{"location":"developers/analyzer-development/","text":"Write analyzers in Timesketch analyzer_run.py Purpose analyzer_run.py is a standalone python script made to bootstrap the development workflow to a minimum where only a file with events and a class file with your analyzer code is needed. You do not have to install Timesketch or any docker for that. running it the first time To be able to run it, you need a python environment with some requirements installed. A good guide to install a venv is published by github here python3 analyzer_run.py usage: analyzer_run.py [-h] [--test_file PATH_TO_TEST_FILE] PATH_TO_ANALYZER NAME_OF_ANALYZER_CLASS analyzer_run.py: error: the following arguments are required: PATH_TO_ANALYZER, NAME_OF_ANALYZER_CLASS create your sample data You can create your sample data either in CSV or JSONL with the same format that Timesketch can ingest. To learn more about that visit CreateTimelineFromJSONorCSV use existing sample data There is an event file shipped with Timesketch at: timesketch/test_tools/test_events/sigma_events.jsonl Running it with parameters The following command PYTHONPATH=. python3 analyzer_run.py --test_file test_events/sigma_events.jsonl ../timesketch/lib/analyzers/sigma_tagger.py RulesSigmaPlugin Will give you that output: -------------------------------------------------------------------------------- sigma -------------------------------------------------------------------------------- Total number of events: 4 Total number of queries: 1 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -- Query #01 -- string: (data_type:(\"shell\\:zsh\\:history\" OR \"bash\\:history\\:command\" OR \"apt\\:history\\:line\") AND \"*apt\\-get\\ install\\ zmap*\") dsl: None indices: ['MOCKED_INDEX'] fields: ['tag', '__ts_emojis', 'human_readable', 'message'] ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Sketch updates: ADD view {'name': '[sigma] Sigma Rule matches', 'query_string': 'tag:\"sigma*\"', 'query_dsl': None, 'query_filter': {'indices': '_all'}, 'additional_fields': None} ADD aggregation {'name': 'Top 20 Sigma tags', 'agg_name': 'field_bucket', 'agg_params': {'field': 'tag', 'limit': 20, 'supported_charts': 'table'}, 'description': 'Created by the Sigma analyzer', 'view_id': 1, 'chart_type': 'hbarchart', 'label': ''} ADD story {'title': 'Sigma Rule hits'} STORY_ADD text {'text': '\\nThis is an automatically generated story that Sigma\\nbased analyzers contribute to.\\n', 'skip_if_exists': True} STORY_ADD text {'text': \"## Sigma Analyzer.\\n\\nThe Sigma analyzer takes Events and matches them with Sigma rules.In this timeline the analyzer discovered 1 Sigma tags.\\n\\nThis is a summary of it's findings.\", 'skip_if_exists': False} STORY_ADD text {'text': 'The top 20 most commonly discovered tags were:', 'skip_if_exists': False} STORY_ADD aggregation {'agg_id': 1, 'agg_name': 'Top 20 Sigma tags', 'agg_type': 'table', 'agg_params': {'field': 'tag', 'limit': 20, 'supported_charts': 'table'}} STORY_ADD text {'text': 'And an overview of all the discovered search terms:', 'skip_if_exists': False} STORY_ADD view {'view_id': 1, 'view_name': '[sigma] Sigma Rule matches'} ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Event Updates: ADD tag [4] ['sigma_lnx_susp_zenmap'] ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Result from analyzer run: Applied 4 tags * lnx_susp_zenmap: 4 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=- Remark Do not try to run analyzer_run.py in your docker instance of Timesketch as it will mix certain things with the actual installed Timesketch instance. Analyzer_run does not actually execute the ES query. Instead all event data passed to the script are assumed to \"match\" the analyzer.","title":"Create an analyzer"},{"location":"developers/analyzer-development/#write-analyzers-in-timesketch","text":"","title":"Write analyzers in Timesketch"},{"location":"developers/analyzer-development/#analyzer_runpy","text":"","title":"analyzer_run.py"},{"location":"developers/analyzer-development/#purpose","text":"analyzer_run.py is a standalone python script made to bootstrap the development workflow to a minimum where only a file with events and a class file with your analyzer code is needed. You do not have to install Timesketch or any docker for that.","title":"Purpose"},{"location":"developers/analyzer-development/#running-it-the-first-time","text":"To be able to run it, you need a python environment with some requirements installed. A good guide to install a venv is published by github here python3 analyzer_run.py usage: analyzer_run.py [-h] [--test_file PATH_TO_TEST_FILE] PATH_TO_ANALYZER NAME_OF_ANALYZER_CLASS analyzer_run.py: error: the following arguments are required: PATH_TO_ANALYZER, NAME_OF_ANALYZER_CLASS","title":"running it the first time"},{"location":"developers/analyzer-development/#create-your-sample-data","text":"You can create your sample data either in CSV or JSONL with the same format that Timesketch can ingest. To learn more about that visit CreateTimelineFromJSONorCSV","title":"create your sample data"},{"location":"developers/analyzer-development/#use-existing-sample-data","text":"There is an event file shipped with Timesketch at: timesketch/test_tools/test_events/sigma_events.jsonl","title":"use existing sample data"},{"location":"developers/analyzer-development/#running-it-with-parameters","text":"The following command PYTHONPATH=. python3 analyzer_run.py --test_file test_events/sigma_events.jsonl ../timesketch/lib/analyzers/sigma_tagger.py RulesSigmaPlugin Will give you that output: -------------------------------------------------------------------------------- sigma -------------------------------------------------------------------------------- Total number of events: 4 Total number of queries: 1 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -- Query #01 -- string: (data_type:(\"shell\\:zsh\\:history\" OR \"bash\\:history\\:command\" OR \"apt\\:history\\:line\") AND \"*apt\\-get\\ install\\ zmap*\") dsl: None indices: ['MOCKED_INDEX'] fields: ['tag', '__ts_emojis', 'human_readable', 'message'] ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Sketch updates: ADD view {'name': '[sigma] Sigma Rule matches', 'query_string': 'tag:\"sigma*\"', 'query_dsl': None, 'query_filter': {'indices': '_all'}, 'additional_fields': None} ADD aggregation {'name': 'Top 20 Sigma tags', 'agg_name': 'field_bucket', 'agg_params': {'field': 'tag', 'limit': 20, 'supported_charts': 'table'}, 'description': 'Created by the Sigma analyzer', 'view_id': 1, 'chart_type': 'hbarchart', 'label': ''} ADD story {'title': 'Sigma Rule hits'} STORY_ADD text {'text': '\\nThis is an automatically generated story that Sigma\\nbased analyzers contribute to.\\n', 'skip_if_exists': True} STORY_ADD text {'text': \"## Sigma Analyzer.\\n\\nThe Sigma analyzer takes Events and matches them with Sigma rules.In this timeline the analyzer discovered 1 Sigma tags.\\n\\nThis is a summary of it's findings.\", 'skip_if_exists': False} STORY_ADD text {'text': 'The top 20 most commonly discovered tags were:', 'skip_if_exists': False} STORY_ADD aggregation {'agg_id': 1, 'agg_name': 'Top 20 Sigma tags', 'agg_type': 'table', 'agg_params': {'field': 'tag', 'limit': 20, 'supported_charts': 'table'}} STORY_ADD text {'text': 'And an overview of all the discovered search terms:', 'skip_if_exists': False} STORY_ADD view {'view_id': 1, 'view_name': '[sigma] Sigma Rule matches'} ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Event Updates: ADD tag [4] ['sigma_lnx_susp_zenmap'] ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Result from analyzer run: Applied 4 tags * lnx_susp_zenmap: 4 =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-","title":"Running it with parameters"},{"location":"developers/analyzer-development/#remark","text":"Do not try to run analyzer_run.py in your docker instance of Timesketch as it will mix certain things with the actual installed Timesketch instance. Analyzer_run does not actually execute the ES query. Instead all event data passed to the script are assumed to \"match\" the analyzer.","title":"Remark"},{"location":"developers/api-client/","text":"API Client The API client is a set of Python libraries that can be used to interact with the REST API of Timesketch from notebooks or scripts. It takes care of setting up authentication, sending the API calls to the server, error handling, and presentation. This documentation will give an overview for the most common use cases of the API client. Some available methods will not be covered in this documentation whereas others will be documented further in a notebook (e.g. colab-timesketch-demo notebook ). Basic Connections The API client defines a config library specifically intended to help with setting up all configuration for connecting to Timesketch, including authentication and server information. The config client will read stored connection information from ~/.timesketchrc , asking the user questions if information is missing (and subsequently storing the results of those questions in the RC file for future lookups). An example use of the config client: from timesketch_api_client import config ts_client = config.get_client() If the configuration file has more than a single section you can define which section to use: from timesketch_api_client import config ts_client = config.get_client(config_section='my_section') To be able to take advantage of the config client, the user needs to be running this from the command line or in a way where questions can be asked and answered (or the config file to be fully populated). This works both in CLI scripts as well as in a notebook. By default the client credentials will be stored on disk in a Token file, in an encrypted format, protected by a randomly generated password that is stored in the RC file. It is highly recommended to protect your credential file using a strong password. The password for the file can be passed on to the config class using: ts_client = config.get_client(token_password='MY_SUPER_L337_PWD') If the token file does not exist, it will be generated and encrypted using the supplied password. Client Config In order to make it simpler to connect to the API client a config file can be generated or created to store information needed to connect. The file is stored in the user's home directory, in a file called $HOME/.timesketchrc . The content of the file is: [timesketch] ... An example config file for an API client that uses OAUTH to connect: [timesketch] client_secret = <redacted secret> host_uri = https://mytimesketchhost.com username = myusername@mydomain.com auth_mode = oauth client_id = <redacted client ID> verify = True A config file can have multiple sections to define which host to connect to. This is useful if you want to be able to connect to more than a single Timesketch instance, for instance your development instance and a production one. An example of that would be: [timesketch] host_uri = https://myprodtimesketch.corp.com username = myselfandirene verify = True client_id = client_secret = auth_mode = userpass cred_key = <redacted> [timesketch-dev] host_uri = http://localhost:5000 username = dev verify = True client_id = client_secret = auth_mode = userpass cred_key = <redacted> token_file_path = /home/myselfandirene/.timesketch.dev.token Each of the additional sections needs to define a separate token file using the token_file_path , otherwise the config will attempt to read the default token file. Using the Timesketch Client The TS client only has limited functionality, such as to list open indices, what sketches the user has access to as well as to fetch sketches, indices, etc. The client object also contains functions to check for OAUTH token status and refresh it, if needed. Most of the functionality of the API client lies in sketch operations. To list all available sketches the function list_sketches can be used: for sketch in ts_client.list_sketches(): # Do something with the sketches... To print the name and description of all available sketches, something like: for sketch in ts_client.list_sketches(): print('[{0:d}] {1:s} <{2:s}>'.format(sketch.id, sketch.name, sketch.description)) Connecting to a Sketch There are two ways of getting a sketch object, either by listing all available sketches or by fetching a specific one. To fetch a specific one use the get_sketch function: sketch = ts_client.get_sketch(SKETCH_ID) All that the function needs is the sketch ID, eg: sketch = ts_client.get_sketch(25) Overview of a Sketch A sketch object has few properties that can be used to get a better overview of what it contains: acl : A python dict with the current sketch ACL. labels : A list of strings, with all the labels that are attached to the sketch. name : The name of the sketch. description : The description of the sketch. status : The status of the sketch. Each of these properties can be accessed using sketch.<PROPERTY> , eg. sketch.name , or sketch.labels Explore Data The sketch object has several ways to explore data, via aggregations or searches. Saved Searches To list all saved searches, use the list_saved_searches function of the sketch object. This functions returns a search.Search object for all saved searches. An example overview would be: for saved_search in sketch.list_saved_searches(): print('{0:03d} - {1:s} [{2:s}]'.format(saved_search.id, saved_search.name, saved_search.user)) To get a particular saved search: saved_search = sketch.get_saved_search(search_id=<SEARCH_ID>) or saved_search = sketch.get_saved_search(search_name=<SEARCH_NAME>) A search object can be used like this: data = saved_search.table The results will be a pandas DataFrame that contains all the records from the saved search. Search Query To search in the API client a search object is used. It will accept several parameters or configurations, for instances a free flowing query string (same as in the UI) or a raw Elastic query DSL. It can also support search chips. The output can be: + A pandas DataFrame. + A python dict. + Stored in a ZIP file. A search object is created from a sketch object. from timesketch_api_client import search ... search_obj = search.Search(sketch=sketch) From there several options are possible: Restore a saved search using from_saved(<SEARCH_ID>) Use the from_manual function that provides several parameters Individually set the needed parameters. The first thing you need to do after creating the object is to configure the search parameters: Retrieve a Stored Search To retrieve a stored search used the from_saved : search_obj.from_saved(<SEARCH_ID>) Configure using the Explore Function It is also possible to configure the search object using the from_manual function. search_obj.from_manual( query_string, query_dsl, query_filter, return_fields, max_entries) All of these parameters are optional, but in order for the search object to be able to query for results you need to provide either query_string or the query_dsl . query_string : This is the Elastic query string, the same one as you would provide in the UI. query_dsl : This is an Elastic Query DSL string. Please see the official documentation about how it is structured. return_fields : This is a comma separated string with all the fields you want to be included in the returned value. If you want all fields returned you can use the wildcard '*'. max_entries : By default the search object returns 10k records maximum. You may want to either reduce that or increase it. If the search has the potential to return more than 10k records a log record will be added indicating how many records there could be, so that the search can be repeated by increasing the max entries. Configure Manually All of the configurations that are present in the from_manual function are also directly available in the object itself, and can be set directly. search_obj.query_string = 'my search' search_obj.max_entries = 10000000 search_obj.return_fields = 'datetime,timestamp_desc,data_type,message,domain,url' There are also other configurations possible. In Jupyter/Colab notebook you can always do search_obj.*? To find a list of all the possible configuration options. Chips Chips are the \"+ Time filter\" or \"+ Add label filter\", etc elements you can see in the UI. There are several chips available: DateIntervalChip DateRangeChip TermChip LabelChip To add a chip to a search object simply use the add_chip function: search_obj.add_chip(label_chip) To view the existing chips: [c.chip for c in search_obj.chips] Or search_obj.query_filter Chips can also be removed using the search_obj.remove_chip(chip_index) function. Each chip will have their own way of configuring it, let's take an example of a date range chip. range_chip = search.DateRangeChip() range_chip.start_time = '2013-09-20T22:20:47' range_chip.end_time = '2013-09-20T22:59:47' search_obj.add_chip(range_chip) For an interval chip: interval_chip = search.DateIntervalChip() interval_chip.date = '2013-09-20T22:39:47' interval_chip.before = 10 interval_chip.after = 10 interval_chip.unit = 'm' search_obj.add_chip(interval_chip) Or a label chip: label_chip = search.LabelChip() label_chip.label = 'foobar' search_obj.add_chip(label_chip) Label chips can also be used to filter out all starred events, or events with comments. For that use: label_chip.use_comment_label() or label_chip.use_star_label() Get The Results Once you have constructed your search object, you may want to explore the results. To get the results as: + a pandas DataFrame use: search_obj.table + a dict, use search_obj.dict + a JSON string, use search_obj.json Or if you want to store the results as a file: search_obj.to_file('/tmp/myresults.zip') (use the ZIP ending, since the resulting file will be a ZIP file with both the results as a CSV file and a METADATA file. Store a Search If you want to store the search query in the datastore, to make it accessible later on, or visible in the UI you can use the save function. First create a name and a description and then save the object. search_obj.name = 'My First Saved Search' search_obj.description = 'This saves all my work' search_obj.save() After you save it, each time you make a change to the search the change gets updated in the datastore. You can also call save() or commit() on the object to make sure it was saved. Aggregations Another option to explore the data is via aggregations. To get a list of available aggregators use: sketch.list_available_aggregators() What gets returned back is a pandas DataFrame with all the aggregators and what parameters they need in order to work. An example aggregation is: params = { 'field': 'domain', 'limit': 10, 'supported_charts': 'hbarchart', 'chart_title': 'Top 10 Domains Visited', } aggregation = sketch.run_aggregator(aggregator_name='field_bucket', aggregator_parameters=params) This will return back an aggregation object that can be used to display the data, eg in a pandas DataFrame df = aggregation.table Or as a chart aggregation.chart And if you want to save that aggregation for future use. aggregation.name = 'TopDomains' aggregation.save() Sigma rules Get a single rule To get a Sigma rule that is stored on the server via uuid of the rule: rule = ts.get_sigma_rule(\"5266a592-b793-11ea-b3de-0242ac130004\") Returns an object, where you can do something like that: rule.data() To get this: { 'title': 'Suspicious Installation of Zenmap', 'id': '5266a592-b793-11ea-b3de-0242ac130004', 'description': 'Detects suspicious installation of Zenmap', 'references': ['https://rmusser.net/docs/ATT&CK-Stuff/ATT&CK/Discovery.html'], 'author': 'Alexander Jaeger', 'date': '2020/06/26', 'modified': '2020/06/26', 'logsource': { 'product': 'linux', 'service': 'shell' }, 'detection': { 'keywords': ['*apt-get install zmap*'], 'condition': 'keywords' }, 'falsepositives': ['Unknown'], 'level': 'high', 'es_query': '(data_type:(\"shell\\\\:zsh\\\\:history\" OR \"bash\\\\:history\\\\:command\" OR \"apt\\\\:history\\\\:line\" OR \"selinux\\\\:line\") AND \"*apt\\\\-get\\\\ install\\\\ zmap*\")', 'file_name': 'lnx_susp_zenmap' } Get a list of rules Another option to explore Sigma rules is via the list_sigma_rules function. To get a list of available Sigma rules use: ts.list_sigma_rules() The output can be: + A pandas DataFrame if the as_pandas=True is set + A python dict (default behavior) Other Options The sketch object can be used to do several other actions that are not documented in this first document, such as: Create/list/retrieve stories Manually add events to the sketch Add timelines to the sketch Modify the ACL of the sketch Archive the sketch (via the sketch.archive function) Comment on an event Delete a sketch Export the sketch data Run analyzers on the sketch. Examples There are several examples using the API client in the notebooks folder in the Github repository.","title":"Introduction"},{"location":"developers/api-client/#api-client","text":"The API client is a set of Python libraries that can be used to interact with the REST API of Timesketch from notebooks or scripts. It takes care of setting up authentication, sending the API calls to the server, error handling, and presentation. This documentation will give an overview for the most common use cases of the API client. Some available methods will not be covered in this documentation whereas others will be documented further in a notebook (e.g. colab-timesketch-demo notebook ).","title":"API Client"},{"location":"developers/api-client/#basic-connections","text":"The API client defines a config library specifically intended to help with setting up all configuration for connecting to Timesketch, including authentication and server information. The config client will read stored connection information from ~/.timesketchrc , asking the user questions if information is missing (and subsequently storing the results of those questions in the RC file for future lookups). An example use of the config client: from timesketch_api_client import config ts_client = config.get_client() If the configuration file has more than a single section you can define which section to use: from timesketch_api_client import config ts_client = config.get_client(config_section='my_section') To be able to take advantage of the config client, the user needs to be running this from the command line or in a way where questions can be asked and answered (or the config file to be fully populated). This works both in CLI scripts as well as in a notebook. By default the client credentials will be stored on disk in a Token file, in an encrypted format, protected by a randomly generated password that is stored in the RC file. It is highly recommended to protect your credential file using a strong password. The password for the file can be passed on to the config class using: ts_client = config.get_client(token_password='MY_SUPER_L337_PWD') If the token file does not exist, it will be generated and encrypted using the supplied password.","title":"Basic Connections"},{"location":"developers/api-client/#client-config","text":"In order to make it simpler to connect to the API client a config file can be generated or created to store information needed to connect. The file is stored in the user's home directory, in a file called $HOME/.timesketchrc . The content of the file is: [timesketch] ... An example config file for an API client that uses OAUTH to connect: [timesketch] client_secret = <redacted secret> host_uri = https://mytimesketchhost.com username = myusername@mydomain.com auth_mode = oauth client_id = <redacted client ID> verify = True A config file can have multiple sections to define which host to connect to. This is useful if you want to be able to connect to more than a single Timesketch instance, for instance your development instance and a production one. An example of that would be: [timesketch] host_uri = https://myprodtimesketch.corp.com username = myselfandirene verify = True client_id = client_secret = auth_mode = userpass cred_key = <redacted> [timesketch-dev] host_uri = http://localhost:5000 username = dev verify = True client_id = client_secret = auth_mode = userpass cred_key = <redacted> token_file_path = /home/myselfandirene/.timesketch.dev.token Each of the additional sections needs to define a separate token file using the token_file_path , otherwise the config will attempt to read the default token file.","title":"Client Config"},{"location":"developers/api-client/#using-the-timesketch-client","text":"The TS client only has limited functionality, such as to list open indices, what sketches the user has access to as well as to fetch sketches, indices, etc. The client object also contains functions to check for OAUTH token status and refresh it, if needed. Most of the functionality of the API client lies in sketch operations. To list all available sketches the function list_sketches can be used: for sketch in ts_client.list_sketches(): # Do something with the sketches... To print the name and description of all available sketches, something like: for sketch in ts_client.list_sketches(): print('[{0:d}] {1:s} <{2:s}>'.format(sketch.id, sketch.name, sketch.description))","title":"Using the Timesketch Client"},{"location":"developers/api-client/#connecting-to-a-sketch","text":"There are two ways of getting a sketch object, either by listing all available sketches or by fetching a specific one. To fetch a specific one use the get_sketch function: sketch = ts_client.get_sketch(SKETCH_ID) All that the function needs is the sketch ID, eg: sketch = ts_client.get_sketch(25)","title":"Connecting to a Sketch"},{"location":"developers/api-client/#overview-of-a-sketch","text":"A sketch object has few properties that can be used to get a better overview of what it contains: acl : A python dict with the current sketch ACL. labels : A list of strings, with all the labels that are attached to the sketch. name : The name of the sketch. description : The description of the sketch. status : The status of the sketch. Each of these properties can be accessed using sketch.<PROPERTY> , eg. sketch.name , or sketch.labels","title":"Overview of a Sketch"},{"location":"developers/api-client/#explore-data","text":"The sketch object has several ways to explore data, via aggregations or searches.","title":"Explore Data"},{"location":"developers/api-client/#saved-searches","text":"To list all saved searches, use the list_saved_searches function of the sketch object. This functions returns a search.Search object for all saved searches. An example overview would be: for saved_search in sketch.list_saved_searches(): print('{0:03d} - {1:s} [{2:s}]'.format(saved_search.id, saved_search.name, saved_search.user)) To get a particular saved search: saved_search = sketch.get_saved_search(search_id=<SEARCH_ID>) or saved_search = sketch.get_saved_search(search_name=<SEARCH_NAME>) A search object can be used like this: data = saved_search.table The results will be a pandas DataFrame that contains all the records from the saved search.","title":"Saved Searches"},{"location":"developers/api-client/#search-query","text":"To search in the API client a search object is used. It will accept several parameters or configurations, for instances a free flowing query string (same as in the UI) or a raw Elastic query DSL. It can also support search chips. The output can be: + A pandas DataFrame. + A python dict. + Stored in a ZIP file. A search object is created from a sketch object. from timesketch_api_client import search ... search_obj = search.Search(sketch=sketch) From there several options are possible: Restore a saved search using from_saved(<SEARCH_ID>) Use the from_manual function that provides several parameters Individually set the needed parameters. The first thing you need to do after creating the object is to configure the search parameters:","title":"Search Query"},{"location":"developers/api-client/#retrieve-a-stored-search","text":"To retrieve a stored search used the from_saved : search_obj.from_saved(<SEARCH_ID>)","title":"Retrieve a Stored Search"},{"location":"developers/api-client/#configure-using-the-explore-function","text":"It is also possible to configure the search object using the from_manual function. search_obj.from_manual( query_string, query_dsl, query_filter, return_fields, max_entries) All of these parameters are optional, but in order for the search object to be able to query for results you need to provide either query_string or the query_dsl . query_string : This is the Elastic query string, the same one as you would provide in the UI. query_dsl : This is an Elastic Query DSL string. Please see the official documentation about how it is structured. return_fields : This is a comma separated string with all the fields you want to be included in the returned value. If you want all fields returned you can use the wildcard '*'. max_entries : By default the search object returns 10k records maximum. You may want to either reduce that or increase it. If the search has the potential to return more than 10k records a log record will be added indicating how many records there could be, so that the search can be repeated by increasing the max entries.","title":"Configure using the Explore Function"},{"location":"developers/api-client/#configure-manually","text":"All of the configurations that are present in the from_manual function are also directly available in the object itself, and can be set directly. search_obj.query_string = 'my search' search_obj.max_entries = 10000000 search_obj.return_fields = 'datetime,timestamp_desc,data_type,message,domain,url' There are also other configurations possible. In Jupyter/Colab notebook you can always do search_obj.*? To find a list of all the possible configuration options.","title":"Configure Manually"},{"location":"developers/api-client/#chips","text":"Chips are the \"+ Time filter\" or \"+ Add label filter\", etc elements you can see in the UI. There are several chips available: DateIntervalChip DateRangeChip TermChip LabelChip To add a chip to a search object simply use the add_chip function: search_obj.add_chip(label_chip) To view the existing chips: [c.chip for c in search_obj.chips] Or search_obj.query_filter Chips can also be removed using the search_obj.remove_chip(chip_index) function. Each chip will have their own way of configuring it, let's take an example of a date range chip. range_chip = search.DateRangeChip() range_chip.start_time = '2013-09-20T22:20:47' range_chip.end_time = '2013-09-20T22:59:47' search_obj.add_chip(range_chip) For an interval chip: interval_chip = search.DateIntervalChip() interval_chip.date = '2013-09-20T22:39:47' interval_chip.before = 10 interval_chip.after = 10 interval_chip.unit = 'm' search_obj.add_chip(interval_chip) Or a label chip: label_chip = search.LabelChip() label_chip.label = 'foobar' search_obj.add_chip(label_chip) Label chips can also be used to filter out all starred events, or events with comments. For that use: label_chip.use_comment_label() or label_chip.use_star_label()","title":"Chips"},{"location":"developers/api-client/#get-the-results","text":"Once you have constructed your search object, you may want to explore the results. To get the results as: + a pandas DataFrame use: search_obj.table + a dict, use search_obj.dict + a JSON string, use search_obj.json Or if you want to store the results as a file: search_obj.to_file('/tmp/myresults.zip') (use the ZIP ending, since the resulting file will be a ZIP file with both the results as a CSV file and a METADATA file.","title":"Get The Results"},{"location":"developers/api-client/#store-a-search","text":"If you want to store the search query in the datastore, to make it accessible later on, or visible in the UI you can use the save function. First create a name and a description and then save the object. search_obj.name = 'My First Saved Search' search_obj.description = 'This saves all my work' search_obj.save() After you save it, each time you make a change to the search the change gets updated in the datastore. You can also call save() or commit() on the object to make sure it was saved.","title":"Store a Search"},{"location":"developers/api-client/#aggregations","text":"Another option to explore the data is via aggregations. To get a list of available aggregators use: sketch.list_available_aggregators() What gets returned back is a pandas DataFrame with all the aggregators and what parameters they need in order to work. An example aggregation is: params = { 'field': 'domain', 'limit': 10, 'supported_charts': 'hbarchart', 'chart_title': 'Top 10 Domains Visited', } aggregation = sketch.run_aggregator(aggregator_name='field_bucket', aggregator_parameters=params) This will return back an aggregation object that can be used to display the data, eg in a pandas DataFrame df = aggregation.table Or as a chart aggregation.chart And if you want to save that aggregation for future use. aggregation.name = 'TopDomains' aggregation.save()","title":"Aggregations"},{"location":"developers/api-client/#sigma-rules","text":"","title":"Sigma rules"},{"location":"developers/api-client/#get-a-single-rule","text":"To get a Sigma rule that is stored on the server via uuid of the rule: rule = ts.get_sigma_rule(\"5266a592-b793-11ea-b3de-0242ac130004\") Returns an object, where you can do something like that: rule.data() To get this: { 'title': 'Suspicious Installation of Zenmap', 'id': '5266a592-b793-11ea-b3de-0242ac130004', 'description': 'Detects suspicious installation of Zenmap', 'references': ['https://rmusser.net/docs/ATT&CK-Stuff/ATT&CK/Discovery.html'], 'author': 'Alexander Jaeger', 'date': '2020/06/26', 'modified': '2020/06/26', 'logsource': { 'product': 'linux', 'service': 'shell' }, 'detection': { 'keywords': ['*apt-get install zmap*'], 'condition': 'keywords' }, 'falsepositives': ['Unknown'], 'level': 'high', 'es_query': '(data_type:(\"shell\\\\:zsh\\\\:history\" OR \"bash\\\\:history\\\\:command\" OR \"apt\\\\:history\\\\:line\" OR \"selinux\\\\:line\") AND \"*apt\\\\-get\\\\ install\\\\ zmap*\")', 'file_name': 'lnx_susp_zenmap' }","title":"Get a single rule"},{"location":"developers/api-client/#get-a-list-of-rules","text":"Another option to explore Sigma rules is via the list_sigma_rules function. To get a list of available Sigma rules use: ts.list_sigma_rules() The output can be: + A pandas DataFrame if the as_pandas=True is set + A python dict (default behavior)","title":"Get a list of rules"},{"location":"developers/api-client/#other-options","text":"The sketch object can be used to do several other actions that are not documented in this first document, such as: Create/list/retrieve stories Manually add events to the sketch Add timelines to the sketch Modify the ACL of the sketch Archive the sketch (via the sketch.archive function) Comment on an event Delete a sketch Export the sketch data Run analyzers on the sketch.","title":"Other Options"},{"location":"developers/api-client/#examples","text":"There are several examples using the API client in the notebooks folder in the Github repository.","title":"Examples"},{"location":"developers/api-upload-data/","text":"Create Timeline From Other Sources Not all data comes in a good CSV or JSONL format that can be imported directly into Timesketch. Your data may lie in a SQL database, Excel sheet, or even in CSV/JSON but it does not have the correct fields in it. In those cases it might be beneficial to have a separate importer in Timesketch that can deal with arbitrary data, for instance if there is already a python library to parse the data, or the data can be read in another format, such as a pandas DataFrame . Disclaimer A small disclaimer. Timesketch is not a parser, and does not intend to be a parser. If you need to parse the data, you'll need other tools or libraries. Parsers can be implemented using plaso . What is the Importer Good For The Timesketch importer is to be used for data sources that you've already parsed or have readily available, but they are not in the correct format that Timesketch requires or you want an automatic way to import the data, or a way to built an importer into your already existing toolsets. This is therefore useful for uploading CSV or JSON files, or through other code that processes data to stream to Timesketch. The importer takes in simple configuration parameters to make the necessary adjustments to the data so that it can be ingested by Timesketch. In the future these adjustments will be configurable using a config file, until then a more manual approach is needed. Basics The importer will take as an input either: Pandas DataFrame. CSV or JSONL. JSON (one JSON per entry) Python dict Microsoft Excel spreadsheet (XLS or XLSX file). The best way to use the streamer is to by using the the with statement in Python, which returns back an object. Before you can use the streamer you'll have to configure it: Add a sketch object to it, this will be the sketch used to upload the data to. Set the name of the imported timeline. If the data does not contain a timestamp description you'll need to set the timestamp_desc field using the streamer.set_timestamp_description . The content of this string will be used for the timestamp_desc field, if it doesn't already exist. If the data does not contain a column called message a format string can be supplied to automatically generate one. This is basically a python formatting string that uses the name of each column as a variable name, eg. \"{src_ip:s} connected to {dst_ip:s}\" means that the content in the column name src_ip will be formatted as a string and replaces the {src_ip:s} in the format string. So if you have a row that contains the variables: src_ip = \"10.10.10.10\", dst_ip = \"8.8.8.8\" then the message string will look like: 10.10.10.10 connected to 8.8.8.8 . The reason why the with statement is preferred is that it ensures that the streamer gets properly closed at the end. The streamer can be used without the with statement, however the developer is then required to make sure that the streamer's .close() function is called at the end. Once the streamer is configured it can be used by calling any of the streamer.add_ functions to add data. Let's look at how to import data using the importer, using each of these data sources. Pandas DataFrame DataFrames can be generated from multiple sources and methods. This documentation is in no way, shape or form going to cover that in any sort of details. There are plenty of guides that can be found online to help you there. Let's just look at a simple case. In [1]: import pandas as pd In [2]: frame = pd.read_excel('~/Documents/SomeRandomDocument.xlsx') In [3]: frame Out[3]: Timestamp What URL Results 0 2019-05-02 23:21:10 Something http://evil.com Nothing to see here 1 2019-05-22 12:12:45 Other http://notevil.com Move on 2 2019-06-23 02:00:12 Not really That http://totallylegit.com Let's not look,shall we Here we have a data frame that we may want to add to our Timesketch instance. What is missing here are few of the necessary columns, see documentation . We don't really need to add them here, we can do that all in our upload stream. Let's start by connecting to a Timesketch instance. import pandas as pd from timesketch_api_client import config from timesketch_import_client import importer ... def action(): frame = pd.read_excel('~/Downloads/SomeRandomDocument.xlsx') ts = config.get_client() my_sketch = ts.get_sketch(SKETCH_ID) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Web Log') streamer.set_timeline_name('excel_import') streamer.set_message_format_string( '{What:s} resulted in {Results:s}, pointed from {URL:s}') streamer.add_data_frame(frame) Python Dict Here is an example of how the streamer can be used to add content using the dictionary approach. Here we use an external library, scapy, to read a PCAP file and import the data from the network traffic to Timesketch. ... from scapy import all as scapy_all ... packets = scapy_all.rdpcap(~/Downloads/SomeRandomDocument.pcap) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Network Log') streamer.set_timeline_name('pcap_test_log') streamer.set_message_format_string( '{src_ip:s}:{src_port:d}->{dst_ip:s}:{dst_port:d} = {url:s}') for packet in packets: # do something here ... timestamp = datetime.datetime.utcfromtimestamp(packet.time) for k, v in iter(data.fields.items()): for url in URL_RE.findall(str(v)): url = url.strip() streamer.add_dict({ 'time': timestamp, 'src_ip': packet.getlayer('IP').src, 'dst_ip': packet.getlayer('IP').dst, 'src_port': layer.sport, 'dst_port': layer.dport, 'url': url}) The streamer will take as an input to add_dict a dictionary that can contain arbitrary field names. These will then later be transformed into a DataFrame and then uploaded to Timesketch. JSON Adding a JSON entry is identical to the dict method, except that the each entry is stored as a separate JSON object (one entry is only a single line). Let's look at an example: # TODO: Add an example. A file, CSV, PLASO or JSONL. Files can also be added using the importer. That is files that are supported by Timesketch. These would be CSV, JSONL (JSON lines) and a plaso file. The function add_file in the importer is used to add a file. Here is an example of how the importer can be used: from timesketch_api_client import config from timesketch_import_client import importer ... with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timeline_name('my_file_with_a_timeline') streamer.set_timestamp_description('some_description') streamer.add_file('/path_to_file/mydump.plaso') If the file that is being imported is either a CSV or a JSONL file the importer will split the file up if it is large and send it in pieces. Each piece of the file will be indexed as soon as it is uploaded to the backend. In the case of a plaso file, it will also be split up into smaller chunks and uploaded. However indexing does not start until all pieces have been transferred and the final plaso storage file reassambled. Excel Sheet from timesketch_api_client import config from timesketch_import_client import importer ... def action(): ts = config.get_client() my_sketch = ts.get_sketch(SKETCH_ID) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Web Log') streamer.set_timeline_name('excel_import') streamer.set_message_format_string( '{What:s} resulted in {Results:s}, pointed from {URL:s}') streamer.add_excel_file('~/Downloads/SomeRandomDocument.xlsx') Import Data Already Ingested into Elastic. You may have other mechanism to ingest data into Elastic, like an ELK stack or some manual scripts that ingest the data. Since the data is already in Elastic it doesn't need to be re-ingested. In order to make it accessible in Timesketch the API client can be used. disclaimer: the data ingested needs to be in a certain format in order to work with Timesketch. This function does limited checking before making it available. The timeline may or may not work in Timesketch, depending on multiple factors. The data that is ingested needs to have few fields already set before it can be ingested into Timesketch: message timestamp datetime The datetime field also needs to be mapped as a date, not a text string. A sample code on how to ingest data into Timesketch that is already in Elastic: from timesketch_api_client import config ts_client = config.get_client() sketch = ts_client.get_sketch(SKETCH_ID) sketch.generate_timeline_from_es_index( es_index_name=ELASTIC_INDEX_NAME, name=TIMELINE_NAME, provider='My Custom Ingestion Script', context='python my_custom_script.py --ingest', )","title":"Upload data via API"},{"location":"developers/api-upload-data/#create-timeline-from-other-sources","text":"Not all data comes in a good CSV or JSONL format that can be imported directly into Timesketch. Your data may lie in a SQL database, Excel sheet, or even in CSV/JSON but it does not have the correct fields in it. In those cases it might be beneficial to have a separate importer in Timesketch that can deal with arbitrary data, for instance if there is already a python library to parse the data, or the data can be read in another format, such as a pandas DataFrame .","title":"Create Timeline From Other Sources"},{"location":"developers/api-upload-data/#disclaimer","text":"A small disclaimer. Timesketch is not a parser, and does not intend to be a parser. If you need to parse the data, you'll need other tools or libraries. Parsers can be implemented using plaso .","title":"Disclaimer"},{"location":"developers/api-upload-data/#what-is-the-importer-good-for","text":"The Timesketch importer is to be used for data sources that you've already parsed or have readily available, but they are not in the correct format that Timesketch requires or you want an automatic way to import the data, or a way to built an importer into your already existing toolsets. This is therefore useful for uploading CSV or JSON files, or through other code that processes data to stream to Timesketch. The importer takes in simple configuration parameters to make the necessary adjustments to the data so that it can be ingested by Timesketch. In the future these adjustments will be configurable using a config file, until then a more manual approach is needed.","title":"What is the Importer Good For"},{"location":"developers/api-upload-data/#basics","text":"The importer will take as an input either: Pandas DataFrame. CSV or JSONL. JSON (one JSON per entry) Python dict Microsoft Excel spreadsheet (XLS or XLSX file). The best way to use the streamer is to by using the the with statement in Python, which returns back an object. Before you can use the streamer you'll have to configure it: Add a sketch object to it, this will be the sketch used to upload the data to. Set the name of the imported timeline. If the data does not contain a timestamp description you'll need to set the timestamp_desc field using the streamer.set_timestamp_description . The content of this string will be used for the timestamp_desc field, if it doesn't already exist. If the data does not contain a column called message a format string can be supplied to automatically generate one. This is basically a python formatting string that uses the name of each column as a variable name, eg. \"{src_ip:s} connected to {dst_ip:s}\" means that the content in the column name src_ip will be formatted as a string and replaces the {src_ip:s} in the format string. So if you have a row that contains the variables: src_ip = \"10.10.10.10\", dst_ip = \"8.8.8.8\" then the message string will look like: 10.10.10.10 connected to 8.8.8.8 . The reason why the with statement is preferred is that it ensures that the streamer gets properly closed at the end. The streamer can be used without the with statement, however the developer is then required to make sure that the streamer's .close() function is called at the end. Once the streamer is configured it can be used by calling any of the streamer.add_ functions to add data. Let's look at how to import data using the importer, using each of these data sources.","title":"Basics"},{"location":"developers/api-upload-data/#pandas-dataframe","text":"DataFrames can be generated from multiple sources and methods. This documentation is in no way, shape or form going to cover that in any sort of details. There are plenty of guides that can be found online to help you there. Let's just look at a simple case. In [1]: import pandas as pd In [2]: frame = pd.read_excel('~/Documents/SomeRandomDocument.xlsx') In [3]: frame Out[3]: Timestamp What URL Results 0 2019-05-02 23:21:10 Something http://evil.com Nothing to see here 1 2019-05-22 12:12:45 Other http://notevil.com Move on 2 2019-06-23 02:00:12 Not really That http://totallylegit.com Let's not look,shall we Here we have a data frame that we may want to add to our Timesketch instance. What is missing here are few of the necessary columns, see documentation . We don't really need to add them here, we can do that all in our upload stream. Let's start by connecting to a Timesketch instance. import pandas as pd from timesketch_api_client import config from timesketch_import_client import importer ... def action(): frame = pd.read_excel('~/Downloads/SomeRandomDocument.xlsx') ts = config.get_client() my_sketch = ts.get_sketch(SKETCH_ID) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Web Log') streamer.set_timeline_name('excel_import') streamer.set_message_format_string( '{What:s} resulted in {Results:s}, pointed from {URL:s}') streamer.add_data_frame(frame)","title":"Pandas DataFrame"},{"location":"developers/api-upload-data/#python-dict","text":"Here is an example of how the streamer can be used to add content using the dictionary approach. Here we use an external library, scapy, to read a PCAP file and import the data from the network traffic to Timesketch. ... from scapy import all as scapy_all ... packets = scapy_all.rdpcap(~/Downloads/SomeRandomDocument.pcap) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Network Log') streamer.set_timeline_name('pcap_test_log') streamer.set_message_format_string( '{src_ip:s}:{src_port:d}->{dst_ip:s}:{dst_port:d} = {url:s}') for packet in packets: # do something here ... timestamp = datetime.datetime.utcfromtimestamp(packet.time) for k, v in iter(data.fields.items()): for url in URL_RE.findall(str(v)): url = url.strip() streamer.add_dict({ 'time': timestamp, 'src_ip': packet.getlayer('IP').src, 'dst_ip': packet.getlayer('IP').dst, 'src_port': layer.sport, 'dst_port': layer.dport, 'url': url}) The streamer will take as an input to add_dict a dictionary that can contain arbitrary field names. These will then later be transformed into a DataFrame and then uploaded to Timesketch.","title":"Python Dict"},{"location":"developers/api-upload-data/#json","text":"Adding a JSON entry is identical to the dict method, except that the each entry is stored as a separate JSON object (one entry is only a single line). Let's look at an example: # TODO: Add an example.","title":"JSON"},{"location":"developers/api-upload-data/#a-file-csv-plaso-or-jsonl","text":"Files can also be added using the importer. That is files that are supported by Timesketch. These would be CSV, JSONL (JSON lines) and a plaso file. The function add_file in the importer is used to add a file. Here is an example of how the importer can be used: from timesketch_api_client import config from timesketch_import_client import importer ... with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timeline_name('my_file_with_a_timeline') streamer.set_timestamp_description('some_description') streamer.add_file('/path_to_file/mydump.plaso') If the file that is being imported is either a CSV or a JSONL file the importer will split the file up if it is large and send it in pieces. Each piece of the file will be indexed as soon as it is uploaded to the backend. In the case of a plaso file, it will also be split up into smaller chunks and uploaded. However indexing does not start until all pieces have been transferred and the final plaso storage file reassambled.","title":"A file, CSV, PLASO or JSONL."},{"location":"developers/api-upload-data/#excel-sheet","text":"from timesketch_api_client import config from timesketch_import_client import importer ... def action(): ts = config.get_client() my_sketch = ts.get_sketch(SKETCH_ID) with importer.ImportStreamer() as streamer: streamer.set_sketch(my_sketch) streamer.set_timestamp_description('Web Log') streamer.set_timeline_name('excel_import') streamer.set_message_format_string( '{What:s} resulted in {Results:s}, pointed from {URL:s}') streamer.add_excel_file('~/Downloads/SomeRandomDocument.xlsx')","title":"Excel Sheet"},{"location":"developers/api-upload-data/#import-data-already-ingested-into-elastic","text":"You may have other mechanism to ingest data into Elastic, like an ELK stack or some manual scripts that ingest the data. Since the data is already in Elastic it doesn't need to be re-ingested. In order to make it accessible in Timesketch the API client can be used. disclaimer: the data ingested needs to be in a certain format in order to work with Timesketch. This function does limited checking before making it available. The timeline may or may not work in Timesketch, depending on multiple factors. The data that is ingested needs to have few fields already set before it can be ingested into Timesketch: message timestamp datetime The datetime field also needs to be mapped as a date, not a text string. A sample code on how to ingest data into Timesketch that is already in Elastic: from timesketch_api_client import config ts_client = config.get_client() sketch = ts_client.get_sketch(SKETCH_ID) sketch.generate_timeline_from_es_index( es_index_name=ELASTIC_INDEX_NAME, name=TIMELINE_NAME, provider='My Custom Ingestion Script', context='python my_custom_script.py --ingest', )","title":"Import Data Already Ingested into Elastic."},{"location":"developers/developer-guide/","text":"Developers guide It is recommended to develop Timesketch using a docker container. Note: Exclamation mark ! denotes commands that should run in the docker container shell, dollar sign $ denotes commands to run in your local shell. Locations and concepts Timesketch provides a webinterface and a REST API The configurations is located at /data sourcecode folder The front end uses Vue.js framework and is stored at /timesketch/frontend Code that is used in potentially multiple locations is stored in /timesketch/lib Analyzers are located at /timesketch/lib/analyzers The API methods are defined in /timesketch/api API client code is in /api_client/python/timesketch_api_client Data models are defined in /timesketch/models Frontend development First we need to get an interactive shell to the container to install the frontend modules: $ docker exec -it $CONTAINER_ID bash Then inside the container shell go to the Timesketch frontend directory. ! cd /usr/local/src/timesketch/timesketch/frontend Note that this directory in the container is mounted as volume from your local repo and mirrors changes to your local repo. Install node dependencies ! npm install This will create node_modules/ folder from package.json in the frontend directory. ! yarn install Running tests and linters The main entry point is run_tests.py in Timesketch root. Please note that for testing and linting python/frontend code in your local environment you need respectively python/ frontend dependencies installed. For more information: ! run_tests.py --help To run frontend tests in watch mode, cd to frontend directory and use ! yarn run test --watch To run TSLint in watch mode, use ! yarn run lint --watch To run a single test (there are multiple ways to do it), open a shell in the docker container: $ docker exec -it $CONTAINER_ID /bin/bash Switch to: ! cd /usr/local/src/timesketch And execute the single test ! nosetests timesketch/lib/emojis_test.py -v Or all in one: $ sudo docker exec -it $CONTAINER_ID nosetests /usr/local/src/timesketch/timesketch/lib/emojis_test.py -v Writing unittests It is recommended to write unittests as much as possible. Test files in Timesketch have the naming convention _test.py and are stored next to the files they test. E.g. a test file for /timesketch/lib/emojis.py is stored as /timesketch/lib/emojis_test.py The unittests for the api client can use mock to emulate responses from the server. The mocked answers are written in: api_client/python/timesketch_api_client/test_lib.py . To introduce a new API endpoint to be tested, the endpoint needs to be registered in the url_router section in /api_client/python/timesketch_api_client/test_lib.py and the response needs to be defined in the same file. end2end tests End2end (e2e) tests are run on Github with every commit. Those tests will setup and run a full Timesketch instance, with the ability to import data and perform actions with it. To run the e2e-tests locally execute to setup the e2e docker images and run them: $ sh end_to_end_tests/tools/run_end_to_end_tests.sh The tests are stored in: end_to_end_tests/*.py And the sample data (currently a plaso file and a csv) is stored in: end_to_end_tests/test_data/ Writing end2end tests While writing end2end tests one approach to make it easier to develop these e2e tests is to create a simlink to the source files, in order for the changes to the files being reflected in the container. Another way is to mount the Timesketch source code from /usr/local/src/timesketch/ to /usr/local/lib/python3.8/dist-packages/ The following example is for changing / adding tests to client_test.py $ export CONTAINER_ID=\"$(sudo -E docker container list -f name=e2e_timesketch -q)\" $ docker exec -it $CONTAINER_ID /bin/bash ! rm /usr/local/lib/python3.8/dist-packages/end_to_end_tests/client_test.py ! ln -s /usr/local/src/timesketch/end_to_end_tests/client_test.py /usr/local/lib/python3.8/dist-packages/end_to_end_tests/client_test.py From now on you can edit the client_test.py file outside of the docker instance and run it again with ! python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py or run the following outside of the container: $ sudo docker exec -it $CONTAINER_ID python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py Building Timesketch frontend To build frontend files and put bundles in timesketch/static/dist/ , use ! yarn run build To watch for changes in code and rebuild automatically, use ! yarn run build --watch This is what you would normally use when making changes to the frontend. Changes are not instantaneous, it takes a couple of seconds to rebuild. It's best to keep this interactive shell to your container running so you can monitor the re-build. Don't forget to refresh page if your browser doesn't automatically load the changes. Packaging Before pushing package to PyPI, make sure you build the frontend before. Local development You may work on the frontend for your local environment for integration with your IDE or other reasons. This is not recommended however as it may cause clashes with your installed NodeJS. Add Node.js 8.x repo $ curl -sS https://deb.nodesource.com/gpgkey/nodesource.gpg.key | sudo apt-key add - $ echo \"deb https://deb.nodesource.com/node_8.x $(lsb_release -s -c) main\" | sudo tee /etc/apt/sources.list.d/nodesource.list Add Yarn repo $ curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - $ echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list ```bash Install Node.js and Yarn ```bash $ apt-get update && apt-get install nodejs yarn After that you would run the same steps as with docker container to install frontend dependencies and build/test. Using Notebook The development container contains a jupyter notebook environment to expirement with the developer instance. To access the notebook access it in a browser using the URL: http://localhost:8844/?token=timesketch (you can also just access http://localhost:8844 and type in timesketch as the password). To get you started there are snippets you can use (look for the snippets drop-down menu and select the code snippet you want to test. To be able to use the notebook container using colab start by creating a notebook and then click the little triangle/arrow button in the upper right corner to connect to a local runtime, see: This will create a pop-up that you need to enter the URL for the local runtime. Use: http://localhost:8844/?token=timesketch as the URL. This will connect to the notebook container, where you can start executing code. There are some things that work better in the Jupyter container though. Developing the API Client Using the Notebook Using the notebook can be very helpful when developing the API client. New features can be easily tested. In order to load changes made in the code, two things need to happen: The code needs to be accessible from the container The code needs to be installed and the kernel restarted For the code to be accessible, it has to be readable by the user with the UID of 1000 or GID of 1000. One way of making sure is to run $ sudo chgrp -R 1000 timesketch Against the source folder. Then inside a notebook to run: !pip install /usr/local/src/timesketch/api_client/python After the code is installed the kernel needs to restarted to make the changes take effect. In the menu select Kernel | Restart , now you should be able to go back into the notebook and make use of the latest changes in the API client. API development Exposing new functionality via the API starts at /timesketch/api/v1/routes.py . In that file the different routes / endpoints are defined that can be used. Typically every route has a dedicated Resource file in /timesketch/api/v1/resources . A resource can have GET as well as POST or other HTTP methods each defined in the same resource file. A good example of a resource that has a mixture is /timesketch/api/v1/resources/archive.py . To write tests for the resource, add a section in /timesketch/api/v1/resources_test.py Error handling It is recommended to expose the error with as much detail as possible to the user / tool that is trying to access the resource. For example the following will give a human readable information as well as a HTTP status code that client code can react on if not sketch: abort(HTTP_STATUS_CODE_NOT_FOUND, 'No sketch found with this ID.') On the opposite side the following is not recommended: if not sketch: abort(HTTP_STATUS_CODE_BAD_REQUEST, 'Error')","title":"Getting started"},{"location":"developers/developer-guide/#developers-guide","text":"It is recommended to develop Timesketch using a docker container. Note: Exclamation mark ! denotes commands that should run in the docker container shell, dollar sign $ denotes commands to run in your local shell.","title":"Developers guide"},{"location":"developers/developer-guide/#locations-and-concepts","text":"Timesketch provides a webinterface and a REST API The configurations is located at /data sourcecode folder The front end uses Vue.js framework and is stored at /timesketch/frontend Code that is used in potentially multiple locations is stored in /timesketch/lib Analyzers are located at /timesketch/lib/analyzers The API methods are defined in /timesketch/api API client code is in /api_client/python/timesketch_api_client Data models are defined in /timesketch/models","title":"Locations and concepts"},{"location":"developers/developer-guide/#frontend-development","text":"First we need to get an interactive shell to the container to install the frontend modules: $ docker exec -it $CONTAINER_ID bash Then inside the container shell go to the Timesketch frontend directory. ! cd /usr/local/src/timesketch/timesketch/frontend Note that this directory in the container is mounted as volume from your local repo and mirrors changes to your local repo. Install node dependencies ! npm install This will create node_modules/ folder from package.json in the frontend directory. ! yarn install","title":"Frontend development"},{"location":"developers/developer-guide/#running-tests-and-linters","text":"The main entry point is run_tests.py in Timesketch root. Please note that for testing and linting python/frontend code in your local environment you need respectively python/ frontend dependencies installed. For more information: ! run_tests.py --help To run frontend tests in watch mode, cd to frontend directory and use ! yarn run test --watch To run TSLint in watch mode, use ! yarn run lint --watch To run a single test (there are multiple ways to do it), open a shell in the docker container: $ docker exec -it $CONTAINER_ID /bin/bash Switch to: ! cd /usr/local/src/timesketch And execute the single test ! nosetests timesketch/lib/emojis_test.py -v Or all in one: $ sudo docker exec -it $CONTAINER_ID nosetests /usr/local/src/timesketch/timesketch/lib/emojis_test.py -v","title":"Running tests and linters"},{"location":"developers/developer-guide/#writing-unittests","text":"It is recommended to write unittests as much as possible. Test files in Timesketch have the naming convention _test.py and are stored next to the files they test. E.g. a test file for /timesketch/lib/emojis.py is stored as /timesketch/lib/emojis_test.py The unittests for the api client can use mock to emulate responses from the server. The mocked answers are written in: api_client/python/timesketch_api_client/test_lib.py . To introduce a new API endpoint to be tested, the endpoint needs to be registered in the url_router section in /api_client/python/timesketch_api_client/test_lib.py and the response needs to be defined in the same file.","title":"Writing unittests"},{"location":"developers/developer-guide/#end2end-tests","text":"End2end (e2e) tests are run on Github with every commit. Those tests will setup and run a full Timesketch instance, with the ability to import data and perform actions with it. To run the e2e-tests locally execute to setup the e2e docker images and run them: $ sh end_to_end_tests/tools/run_end_to_end_tests.sh The tests are stored in: end_to_end_tests/*.py And the sample data (currently a plaso file and a csv) is stored in: end_to_end_tests/test_data/","title":"end2end tests"},{"location":"developers/developer-guide/#writing-end2end-tests","text":"While writing end2end tests one approach to make it easier to develop these e2e tests is to create a simlink to the source files, in order for the changes to the files being reflected in the container. Another way is to mount the Timesketch source code from /usr/local/src/timesketch/ to /usr/local/lib/python3.8/dist-packages/ The following example is for changing / adding tests to client_test.py $ export CONTAINER_ID=\"$(sudo -E docker container list -f name=e2e_timesketch -q)\" $ docker exec -it $CONTAINER_ID /bin/bash ! rm /usr/local/lib/python3.8/dist-packages/end_to_end_tests/client_test.py ! ln -s /usr/local/src/timesketch/end_to_end_tests/client_test.py /usr/local/lib/python3.8/dist-packages/end_to_end_tests/client_test.py From now on you can edit the client_test.py file outside of the docker instance and run it again with ! python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py or run the following outside of the container: $ sudo docker exec -it $CONTAINER_ID python3 /usr/local/src/timesketch/end_to_end_tests/tools/run_in_container.py","title":"Writing end2end tests"},{"location":"developers/developer-guide/#building-timesketch-frontend","text":"To build frontend files and put bundles in timesketch/static/dist/ , use ! yarn run build To watch for changes in code and rebuild automatically, use ! yarn run build --watch This is what you would normally use when making changes to the frontend. Changes are not instantaneous, it takes a couple of seconds to rebuild. It's best to keep this interactive shell to your container running so you can monitor the re-build. Don't forget to refresh page if your browser doesn't automatically load the changes.","title":"Building Timesketch frontend"},{"location":"developers/developer-guide/#packaging","text":"Before pushing package to PyPI, make sure you build the frontend before.","title":"Packaging"},{"location":"developers/developer-guide/#local-development","text":"You may work on the frontend for your local environment for integration with your IDE or other reasons. This is not recommended however as it may cause clashes with your installed NodeJS. Add Node.js 8.x repo $ curl -sS https://deb.nodesource.com/gpgkey/nodesource.gpg.key | sudo apt-key add - $ echo \"deb https://deb.nodesource.com/node_8.x $(lsb_release -s -c) main\" | sudo tee /etc/apt/sources.list.d/nodesource.list Add Yarn repo $ curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - $ echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list ```bash Install Node.js and Yarn ```bash $ apt-get update && apt-get install nodejs yarn After that you would run the same steps as with docker container to install frontend dependencies and build/test.","title":"Local development"},{"location":"developers/developer-guide/#using-notebook","text":"The development container contains a jupyter notebook environment to expirement with the developer instance. To access the notebook access it in a browser using the URL: http://localhost:8844/?token=timesketch (you can also just access http://localhost:8844 and type in timesketch as the password). To get you started there are snippets you can use (look for the snippets drop-down menu and select the code snippet you want to test. To be able to use the notebook container using colab start by creating a notebook and then click the little triangle/arrow button in the upper right corner to connect to a local runtime, see: This will create a pop-up that you need to enter the URL for the local runtime. Use: http://localhost:8844/?token=timesketch as the URL. This will connect to the notebook container, where you can start executing code. There are some things that work better in the Jupyter container though.","title":"Using Notebook"},{"location":"developers/developer-guide/#developing-the-api-client-using-the-notebook","text":"Using the notebook can be very helpful when developing the API client. New features can be easily tested. In order to load changes made in the code, two things need to happen: The code needs to be accessible from the container The code needs to be installed and the kernel restarted For the code to be accessible, it has to be readable by the user with the UID of 1000 or GID of 1000. One way of making sure is to run $ sudo chgrp -R 1000 timesketch Against the source folder. Then inside a notebook to run: !pip install /usr/local/src/timesketch/api_client/python After the code is installed the kernel needs to restarted to make the changes take effect. In the menu select Kernel | Restart , now you should be able to go back into the notebook and make use of the latest changes in the API client.","title":"Developing the API Client Using the Notebook"},{"location":"developers/developer-guide/#api-development","text":"Exposing new functionality via the API starts at /timesketch/api/v1/routes.py . In that file the different routes / endpoints are defined that can be used. Typically every route has a dedicated Resource file in /timesketch/api/v1/resources . A resource can have GET as well as POST or other HTTP methods each defined in the same resource file. A good example of a resource that has a mixture is /timesketch/api/v1/resources/archive.py . To write tests for the resource, add a section in /timesketch/api/v1/resources_test.py","title":"API development"},{"location":"developers/developer-guide/#error-handling","text":"It is recommended to expose the error with as much detail as possible to the user / tool that is trying to access the resource. For example the following will give a human readable information as well as a HTTP status code that client code can react on if not sketch: abort(HTTP_STATUS_CODE_NOT_FOUND, 'No sketch found with this ID.') On the opposite side the following is not recommended: if not sketch: abort(HTTP_STATUS_CODE_BAD_REQUEST, 'Error')","title":"Error handling"},{"location":"getting-started/install/","text":"Install Timesketch The preferred way to install Timesketch is to use the provided Docker images. These docker images are automatically built whenever the main branch is updated or a new release is tagged. It is possible to install Timesketch without docker but we strongly enocurage using docker. This is the only tested and actively maintained installation method. You will need Machine with Ubuntu 20.04 installed. At least 8GB RAM, but more the better. Optional: Domain name registered and configure for the machine if you want to setup SSL for the webserver. This guide setup the following services Timesketch web/api server Timesketch importer/analysis worker PostgreSQL database Elasticsearch single-node cluster Redis key-value database (for worker processes) Nginx webserver NOTE : This guide sets up single node Elasticsearch cluster. This is OK for smaller installations but in order to scale and have better performance you need to setup a multi node Elasticsearch cluster. This is out of scope for this guide but the official documentation on installing Elasticsearch with Docker will get you started. 1. Install Docker Follow the official installation instructions to install Docker Engine on Ubuntu . Make sure you install docker-compose as well sudo apt install docker-compose 2. Start the installation Download helper script We have created a helper script to get you started with all necessary configuration. Download the script here: curl -s -O https://raw.githubusercontent.com/google/timesketch/master/contrib/deploy_timesketch.sh chmod 755 deploy_timesketch.sh Choose location for the installation You can choose to host the Timeksetch data directory anywhere but note that by default it will host Elasticsearch and PostgreSQL data in this directory so make sure you have enough disk space available. Example: cd /opt Run deployment script sudo ~/deploy_timesketch.sh 3. Start the system cd timesketch sudo docker-compose up -d Create the first user sudo docker-compose exec timesketch-web tsctl add_user --username <USERNAME> 4. Enable TLS (optional) It is out of scope for the deployment script to setup certificates but here are pointers on how to use Let's Encrypt. You need to configure a DNS name for the server. Use your DNS provider instructions. Make sure your webserver is reachable on port 80. Follow the official guide to install and run Let's Encrypt on Ubuntu: https://certbot.eff.org/lets-encrypt/ubuntufocal-other When Let's Encrypt has been installed and you have generated certificates (located in /etc/letsencrypt) it is time to reconfigure Nginx. Edit timesketch/etc/nginx.conf (HOSTNAME is the DNS name of your server): events { worker_connections 768; } http { server { listen 80; listen [::]:80; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/<HOSTNAME>>/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/<HOSTNAME>>/privkey.pem; client_max_body_size 0m; location / { proxy_buffer_size 128k; proxy_buffers 4 256k; proxy_busy_buffers_size 256k; proxy_pass http://timesketch-web:5000; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } if ($scheme != \"https\") { return 301 https://$host$request_uri; } } } Make the certificate and key available to the Nginx Docker container. Edit timesketch/docker-compose.yml and mount /etc/letsencrypt: ... nginx: image: nginx:${NGINX_VERSION} restart: always ports: - \"80:80\" - \"443:443\" volumes: - ./etc/nginx.conf:/etc/nginx/nginx.conf - /etc/letsencrypt:/etc/letsencrypt/ Restart the system: docker-compose down docker-compose up -d Congratulations, your Timesketch system is operational and ready to use.","title":"Installation"},{"location":"getting-started/install/#install-timesketch","text":"The preferred way to install Timesketch is to use the provided Docker images. These docker images are automatically built whenever the main branch is updated or a new release is tagged. It is possible to install Timesketch without docker but we strongly enocurage using docker. This is the only tested and actively maintained installation method. You will need Machine with Ubuntu 20.04 installed. At least 8GB RAM, but more the better. Optional: Domain name registered and configure for the machine if you want to setup SSL for the webserver. This guide setup the following services Timesketch web/api server Timesketch importer/analysis worker PostgreSQL database Elasticsearch single-node cluster Redis key-value database (for worker processes) Nginx webserver NOTE : This guide sets up single node Elasticsearch cluster. This is OK for smaller installations but in order to scale and have better performance you need to setup a multi node Elasticsearch cluster. This is out of scope for this guide but the official documentation on installing Elasticsearch with Docker will get you started.","title":"Install Timesketch"},{"location":"getting-started/install/#1-install-docker","text":"Follow the official installation instructions to install Docker Engine on Ubuntu . Make sure you install docker-compose as well sudo apt install docker-compose","title":"1. Install Docker"},{"location":"getting-started/install/#2-start-the-installation","text":"","title":"2. Start the installation"},{"location":"getting-started/install/#download-helper-script","text":"We have created a helper script to get you started with all necessary configuration. Download the script here: curl -s -O https://raw.githubusercontent.com/google/timesketch/master/contrib/deploy_timesketch.sh chmod 755 deploy_timesketch.sh","title":"Download helper script"},{"location":"getting-started/install/#choose-location-for-the-installation","text":"You can choose to host the Timeksetch data directory anywhere but note that by default it will host Elasticsearch and PostgreSQL data in this directory so make sure you have enough disk space available. Example: cd /opt","title":"Choose location for the installation"},{"location":"getting-started/install/#run-deployment-script","text":"sudo ~/deploy_timesketch.sh","title":"Run deployment script"},{"location":"getting-started/install/#3-start-the-system","text":"cd timesketch sudo docker-compose up -d","title":"3. Start the system"},{"location":"getting-started/install/#create-the-first-user","text":"sudo docker-compose exec timesketch-web tsctl add_user --username <USERNAME>","title":"Create the first user"},{"location":"getting-started/install/#4-enable-tls-optional","text":"It is out of scope for the deployment script to setup certificates but here are pointers on how to use Let's Encrypt. You need to configure a DNS name for the server. Use your DNS provider instructions. Make sure your webserver is reachable on port 80. Follow the official guide to install and run Let's Encrypt on Ubuntu: https://certbot.eff.org/lets-encrypt/ubuntufocal-other When Let's Encrypt has been installed and you have generated certificates (located in /etc/letsencrypt) it is time to reconfigure Nginx. Edit timesketch/etc/nginx.conf (HOSTNAME is the DNS name of your server): events { worker_connections 768; } http { server { listen 80; listen [::]:80; listen 443 ssl; ssl_certificate /etc/letsencrypt/live/<HOSTNAME>>/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/<HOSTNAME>>/privkey.pem; client_max_body_size 0m; location / { proxy_buffer_size 128k; proxy_buffers 4 256k; proxy_busy_buffers_size 256k; proxy_pass http://timesketch-web:5000; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } if ($scheme != \"https\") { return 301 https://$host$request_uri; } } } Make the certificate and key available to the Nginx Docker container. Edit timesketch/docker-compose.yml and mount /etc/letsencrypt: ... nginx: image: nginx:${NGINX_VERSION} restart: always ports: - \"80:80\" - \"443:443\" volumes: - ./etc/nginx.conf:/etc/nginx/nginx.conf - /etc/letsencrypt:/etc/letsencrypt/ Restart the system: docker-compose down docker-compose up -d Congratulations, your Timesketch system is operational and ready to use.","title":"4. Enable TLS (optional)"},{"location":"getting-started/upload-data/","text":"Upload Data to Timesketch There are several different ways to upload data to Timesketch. This document attempts to explore them all. These are the different ways to upload data: Using the importer CLI tool Using the web UI Using the importer library Let's explore each of these ways a bit further. Using the importer CLI Tool If the data that is to be imported is a single file then the importer tool can be used. It utilizes the importer client library and the API client to upload the file. This is a simple wrapper around the importer client libraries. The tool comes with the installation of the timesketch importer client. Install the tool $ pip3 install timesketch-import-client There are two methods to use the tool: Define all parameters on the command line. The preferred method of just running the tool omitting all information about the authentication and/or server information and have the tool ask all the questions. The easiest way to discover the parameters and how to run the tool is to run: $ timesketch_importer.py -h The minimum set of parameters to the run tool are: $ timesketch_importer.py path_to_my_file.csv If the information to connect to Timesketch are not present (host information, auth method and auth information) then the tool will ask the user for the missing information and store it in the configuration file ~/.timesketchrc for future use. Remember for OAUTH authentication both client_id and client_secret need to provided to the tool. The tool will store the user credentials in an encrypted file as soon as it runs for the first time. This token file will be used for subsequent uses of the tool. Other parameters suggested to be set are sketch_id (if it isn't provided a new sketch will be created) and timeline_name (otherwise a default name will be chosen). For larger files the importer will split them up into pieces before uploading. Using the Web UI Click the + timeline button in the UI or click manage in the Timeline section and then add your timeline using the Choose file button that appears below the timelines. Using the importer library The importer client defines an importer library that is used to help with file or data uploads. This is documented further here","title":"Upload data"},{"location":"getting-started/upload-data/#upload-data-to-timesketch","text":"There are several different ways to upload data to Timesketch. This document attempts to explore them all. These are the different ways to upload data: Using the importer CLI tool Using the web UI Using the importer library Let's explore each of these ways a bit further.","title":"Upload Data to Timesketch"},{"location":"getting-started/upload-data/#using-the-importer-cli-tool","text":"If the data that is to be imported is a single file then the importer tool can be used. It utilizes the importer client library and the API client to upload the file. This is a simple wrapper around the importer client libraries. The tool comes with the installation of the timesketch importer client.","title":"Using the importer CLI Tool"},{"location":"getting-started/upload-data/#install-the-tool","text":"$ pip3 install timesketch-import-client There are two methods to use the tool: Define all parameters on the command line. The preferred method of just running the tool omitting all information about the authentication and/or server information and have the tool ask all the questions. The easiest way to discover the parameters and how to run the tool is to run: $ timesketch_importer.py -h The minimum set of parameters to the run tool are: $ timesketch_importer.py path_to_my_file.csv If the information to connect to Timesketch are not present (host information, auth method and auth information) then the tool will ask the user for the missing information and store it in the configuration file ~/.timesketchrc for future use. Remember for OAUTH authentication both client_id and client_secret need to provided to the tool. The tool will store the user credentials in an encrypted file as soon as it runs for the first time. This token file will be used for subsequent uses of the tool. Other parameters suggested to be set are sketch_id (if it isn't provided a new sketch will be created) and timeline_name (otherwise a default name will be chosen). For larger files the importer will split them up into pieces before uploading.","title":"Install the tool"},{"location":"getting-started/upload-data/#using-the-web-ui","text":"Click the + timeline button in the UI or click manage in the Timeline section and then add your timeline using the Choose file button that appears below the timelines.","title":"Using the Web UI"},{"location":"getting-started/upload-data/#using-the-importer-library","text":"The importer client defines an importer library that is used to help with file or data uploads. This is documented further here","title":"Using the importer library"},{"location":"learn/basic-concepts/","text":"Concepts Timesketch is built on multiple sketches, where one sketch is usually one case. Every sketch can consist of multiple timelines with multiple views. Login Use the credentials provided by your Timesketch admin to log on to Timesketch or use OAuth to authenticate. Sketches There is a dedicated document to walk you through Sketches Adding Timelines Create timeline from JSON/JSONL/CSV file Upload data via the importer/API Enable Plaso upload via HTTP Adding event This feature is currently not implemented in the Web UI. But you can add events using the API client . Add a comment You can comment events in your sketch. The comments are safed in your sketch, that means if you add a timeline to multiple sketches, the comments are only shown in the one sketch you made the comments. Star an event Click the little star symbol in the Event List to star an event. Stared events can be used to filter on them and or to add all starred events in your story. Views Views are saved search queries. Those can either be created by the User, by API or via Analyzers. To create a view from the Web Ui, click the Save as view button on the top right of the Search fields in the Explore Tab of a sketch. Insights / Aggegations The Insights functionality in a sketch gives the opportunity to run aggregations on the events in a sketch. There are currently two aggregators available: - Terms aggregation - Filtered terms aggregation Terms aggregation The term aggregation can be used for example to get a table view of the present data types in the sketch. You can choose between the following chart types: circlechart table barchart hbchart linechart The next value to provide is the field you want to aggregate on, for example the data_type. The last value is the number of results to return. Once the aggregation is completed, you can save the aggregation by clicking the save button on the top right corner of the aggregation result, for example to add it to a story. Filtered terms aggregation The filtered terms aggregation works the same way the terms aggregation works with one additional input field the filter query. This can be used for example to aggregate over data_types only for events that contain a certain string. Customize columns In the Explore view of a sketch the message is the only column visible. To add more columns, click the customize columns* button on the top right of the events list. The list of available columns is pre-populated from the columns in your timeline. Stories A story is a place where you can capture the narrative of your technical investigation and add detail to your story with raw timeline data and aggregated data. The editor lets you to write and capture the story behind your investigation and at the same time enable you to share detailed findings without spending hours writing reports. You can add events from previously saved searches. Just hit enter to start a new paragraph and choose the saved search from the dropdown menu. See Medium article You can add saved views, aggregations and text in markdown format to a story. Some analyzers automatically generate stories to either highlight possible events of interest or to document their runtime. If you want to export a story, export the whole Sketch. The zip file will contain each story. A story can also be exported using the API Client . Demo To play with timesketch without any installation visit demo.timesketch.org as demo/demo . Searching There is a dedicated document called SearchQueryGuide to help you create custom searches. All data within Timesketch is stored in elasticsearch. So the search works similar to ES. Using the advances search, a JSON can be passed to Timesketch { \"query\": { \"bool\": { \"must\": [ { \"query_string\": { \"query\": \"*\" } } ] } }, \"sort\": { \"datetime\": \"asc\" } } Analyzers With Analyzers you can enrich your data in timelines. The analysers are written in Python. The system consist of a set of background workers that can execute Python code on a stream of events. It provides an easy to use API to programmatically do all the actions available in the UI, e.g. tagging events, star and create saved views etc. The idea is to automatically enrich data with encoded analysis knowledge. The code for Analyzers is located at /timesketch/lib/analyzers Analyzer description Note: Not all analyzers are explained in this documentation. If you have questions to a particular analyzer, please have a look at the code or file an Issue on Github . Browser Search Analyzer The browser search analyzer takes URLs usually reserved for browser search queries and extracts the search string. It will also tell in a story: The top 20 most commonly discovered searches The domains used to search The most common days of search And an overview of all the discovered search terms Browser Timeframe Analyzer The browser time frame analyzer discovers browser events that occurred outside of the typical browsing window of this browser history. The analyzer determines the activity hours by finding the frequency of browsing events per hour, and then discovering the longest block of most active hours before proceeding with flagging all events outside of that time period. This information can be used by other analyzers or by manually looking for other activity within the inactive time period to find unusual actions. Chain analyzer Sketch analyzer for chained events The purpose of the chain analyzer is to chain together events that can be described as linked, either by sharing some common entities, or one event being a derivative of another event. An example of this would be that a browser downloads an executable, which then later gets executed. The signs of execution could lie in multiple events, from different sources, but they are all linked or chained together. This could help an analyst see the connection between these separate but chained events. Another example could be a document written and then compressed into a ZIP file, which would then be exfilled through some means. If the document and the ZIP file are chained together it could be easier for the analyst to track the meaning of an exfil event involving the compressed file. Domain Analyzer The Domain Analyzer extracts domain and Top Level Domain (TLD) info from events that have a field with either url or domain . It will also add information about: Known CDN providers (based on Timesketch config) Windows EVTX Sessionizer This Analyzer will determine the start and end event for a user session based on EVTX events. Windows EVTX Gap analyzer It attempts to detect gaps in EVTX files found in an index using two different methods. First of all it looks at missing entries in record numbers and secondly it attempts to look at gaps in days with no records. This may be an indication of someone clearing the logs, yet it may be an indication of something else. At least this should be interpreted as something that warrants a second look. This will obviously not catch every instance of someone clearing EVTX records, even if that's done in bulk. Therefore it should not be interpreted that if this analyzer does not discover something that the records have not been wiped. Please verify the results given by this analyzer. The results of this analyzer will be a story, that details the findings. Safebrowsing Analyzer This Analyzer checks urls found in a sketch against the Google Safebrowsing API . To use this Analyzer, the following parameter must be set in the timesketch.conf : SAFEBROWSING_API_KEY = '' This analyzer can be customized by creating an optional file containing URL wildcards to be allow listed called safebrowsing_allowlist.yaml There are also two additional config parameters, please refer to the Safe Browsing API reference . Platforms to be looked at in Safe Browsing (PlatformType). SAFEBROWSING_PLATFORMS = ['ANY_PLATFORM'] Types to be looked at in Safe Browsing (ThreatType). SAFEBROWSING_THREATTYPES = ['MALWARE'] Sigma Analyzer The Sigma Analyzer translates Sigma rules in Elastic Search Queries and adds a tag to every matching event. It will also create a story with the Top 10 matched Sigma rules. There is a dedicated document to walk you through the process of using the Sigma Analyzer . YetiIndicators Analyzer This is a Index analyzer for Yeti threat intel indicators. To use this Analyzer, the following parameter must be set with corresponding values in the timesketch.conf : YETI_API_ROOT = '' YETI_API_KEY = ''","title":"Basic concepts"},{"location":"learn/basic-concepts/#concepts","text":"Timesketch is built on multiple sketches, where one sketch is usually one case. Every sketch can consist of multiple timelines with multiple views.","title":"Concepts"},{"location":"learn/basic-concepts/#login","text":"Use the credentials provided by your Timesketch admin to log on to Timesketch or use OAuth to authenticate.","title":"Login"},{"location":"learn/basic-concepts/#sketches","text":"There is a dedicated document to walk you through Sketches","title":"Sketches"},{"location":"learn/basic-concepts/#adding-timelines","text":"Create timeline from JSON/JSONL/CSV file Upload data via the importer/API Enable Plaso upload via HTTP","title":"Adding Timelines"},{"location":"learn/basic-concepts/#adding-event","text":"This feature is currently not implemented in the Web UI. But you can add events using the API client .","title":"Adding event"},{"location":"learn/basic-concepts/#add-a-comment","text":"You can comment events in your sketch. The comments are safed in your sketch, that means if you add a timeline to multiple sketches, the comments are only shown in the one sketch you made the comments.","title":"Add a comment"},{"location":"learn/basic-concepts/#star-an-event","text":"Click the little star symbol in the Event List to star an event. Stared events can be used to filter on them and or to add all starred events in your story.","title":"Star an event"},{"location":"learn/basic-concepts/#views","text":"Views are saved search queries. Those can either be created by the User, by API or via Analyzers. To create a view from the Web Ui, click the Save as view button on the top right of the Search fields in the Explore Tab of a sketch.","title":"Views"},{"location":"learn/basic-concepts/#insights-aggegations","text":"The Insights functionality in a sketch gives the opportunity to run aggregations on the events in a sketch. There are currently two aggregators available: - Terms aggregation - Filtered terms aggregation","title":"Insights / Aggegations"},{"location":"learn/basic-concepts/#terms-aggregation","text":"The term aggregation can be used for example to get a table view of the present data types in the sketch. You can choose between the following chart types: circlechart table barchart hbchart linechart The next value to provide is the field you want to aggregate on, for example the data_type. The last value is the number of results to return. Once the aggregation is completed, you can save the aggregation by clicking the save button on the top right corner of the aggregation result, for example to add it to a story.","title":"Terms aggregation"},{"location":"learn/basic-concepts/#filtered-terms-aggregation","text":"The filtered terms aggregation works the same way the terms aggregation works with one additional input field the filter query. This can be used for example to aggregate over data_types only for events that contain a certain string.","title":"Filtered terms aggregation"},{"location":"learn/basic-concepts/#customize-columns","text":"In the Explore view of a sketch the message is the only column visible. To add more columns, click the customize columns* button on the top right of the events list. The list of available columns is pre-populated from the columns in your timeline.","title":"Customize columns"},{"location":"learn/basic-concepts/#stories","text":"A story is a place where you can capture the narrative of your technical investigation and add detail to your story with raw timeline data and aggregated data. The editor lets you to write and capture the story behind your investigation and at the same time enable you to share detailed findings without spending hours writing reports. You can add events from previously saved searches. Just hit enter to start a new paragraph and choose the saved search from the dropdown menu. See Medium article You can add saved views, aggregations and text in markdown format to a story. Some analyzers automatically generate stories to either highlight possible events of interest or to document their runtime. If you want to export a story, export the whole Sketch. The zip file will contain each story. A story can also be exported using the API Client .","title":"Stories"},{"location":"learn/basic-concepts/#demo","text":"To play with timesketch without any installation visit demo.timesketch.org as demo/demo .","title":"Demo"},{"location":"learn/basic-concepts/#searching","text":"There is a dedicated document called SearchQueryGuide to help you create custom searches. All data within Timesketch is stored in elasticsearch. So the search works similar to ES. Using the advances search, a JSON can be passed to Timesketch { \"query\": { \"bool\": { \"must\": [ { \"query_string\": { \"query\": \"*\" } } ] } }, \"sort\": { \"datetime\": \"asc\" } }","title":"Searching"},{"location":"learn/basic-concepts/#analyzers","text":"With Analyzers you can enrich your data in timelines. The analysers are written in Python. The system consist of a set of background workers that can execute Python code on a stream of events. It provides an easy to use API to programmatically do all the actions available in the UI, e.g. tagging events, star and create saved views etc. The idea is to automatically enrich data with encoded analysis knowledge. The code for Analyzers is located at /timesketch/lib/analyzers","title":"Analyzers"},{"location":"learn/basic-concepts/#analyzer-description","text":"Note: Not all analyzers are explained in this documentation. If you have questions to a particular analyzer, please have a look at the code or file an Issue on Github .","title":"Analyzer description"},{"location":"learn/basic-concepts/#browser-search-analyzer","text":"The browser search analyzer takes URLs usually reserved for browser search queries and extracts the search string. It will also tell in a story: The top 20 most commonly discovered searches The domains used to search The most common days of search And an overview of all the discovered search terms","title":"Browser Search Analyzer"},{"location":"learn/basic-concepts/#browser-timeframe-analyzer","text":"The browser time frame analyzer discovers browser events that occurred outside of the typical browsing window of this browser history. The analyzer determines the activity hours by finding the frequency of browsing events per hour, and then discovering the longest block of most active hours before proceeding with flagging all events outside of that time period. This information can be used by other analyzers or by manually looking for other activity within the inactive time period to find unusual actions.","title":"Browser Timeframe Analyzer"},{"location":"learn/basic-concepts/#chain-analyzer","text":"Sketch analyzer for chained events The purpose of the chain analyzer is to chain together events that can be described as linked, either by sharing some common entities, or one event being a derivative of another event. An example of this would be that a browser downloads an executable, which then later gets executed. The signs of execution could lie in multiple events, from different sources, but they are all linked or chained together. This could help an analyst see the connection between these separate but chained events. Another example could be a document written and then compressed into a ZIP file, which would then be exfilled through some means. If the document and the ZIP file are chained together it could be easier for the analyst to track the meaning of an exfil event involving the compressed file.","title":"Chain analyzer"},{"location":"learn/basic-concepts/#domain-analyzer","text":"The Domain Analyzer extracts domain and Top Level Domain (TLD) info from events that have a field with either url or domain . It will also add information about: Known CDN providers (based on Timesketch config)","title":"Domain Analyzer"},{"location":"learn/basic-concepts/#windows-evtx-sessionizer","text":"This Analyzer will determine the start and end event for a user session based on EVTX events.","title":"Windows EVTX Sessionizer"},{"location":"learn/basic-concepts/#windows-evtx-gap-analyzer","text":"It attempts to detect gaps in EVTX files found in an index using two different methods. First of all it looks at missing entries in record numbers and secondly it attempts to look at gaps in days with no records. This may be an indication of someone clearing the logs, yet it may be an indication of something else. At least this should be interpreted as something that warrants a second look. This will obviously not catch every instance of someone clearing EVTX records, even if that's done in bulk. Therefore it should not be interpreted that if this analyzer does not discover something that the records have not been wiped. Please verify the results given by this analyzer. The results of this analyzer will be a story, that details the findings.","title":"Windows EVTX Gap analyzer"},{"location":"learn/basic-concepts/#safebrowsing-analyzer","text":"This Analyzer checks urls found in a sketch against the Google Safebrowsing API . To use this Analyzer, the following parameter must be set in the timesketch.conf : SAFEBROWSING_API_KEY = '' This analyzer can be customized by creating an optional file containing URL wildcards to be allow listed called safebrowsing_allowlist.yaml There are also two additional config parameters, please refer to the Safe Browsing API reference . Platforms to be looked at in Safe Browsing (PlatformType). SAFEBROWSING_PLATFORMS = ['ANY_PLATFORM'] Types to be looked at in Safe Browsing (ThreatType). SAFEBROWSING_THREATTYPES = ['MALWARE']","title":"Safebrowsing Analyzer"},{"location":"learn/basic-concepts/#sigma-analyzer","text":"The Sigma Analyzer translates Sigma rules in Elastic Search Queries and adds a tag to every matching event. It will also create a story with the Top 10 matched Sigma rules. There is a dedicated document to walk you through the process of using the Sigma Analyzer .","title":"Sigma Analyzer"},{"location":"learn/basic-concepts/#yetiindicators-analyzer","text":"This is a Index analyzer for Yeti threat intel indicators. To use this Analyzer, the following parameter must be set with corresponding values in the timesketch.conf : YETI_API_ROOT = '' YETI_API_KEY = ''","title":"YetiIndicators Analyzer"},{"location":"learn/create-timeline-from-json-csv/","text":"Create timeline from JSONL or CSV file You can ingest timeline data from a JSONL or CSV file. You can have any number of attributes/columns as you wish but there are some mandatory fields that Timeksketch needs in order to render the events in the UI. Mandatory fields: message String with an informative message of the event datetime ISO8601 format Ex: 2015-07-24T19:01:01+00:00 timestamp_desc String explaining what type of timestamp it is. E.g file created Ex: \"Time created\" Example CSV file You need to provide the CSV header with the column names as the first line in the file. message,timestamp,datetime,timestamp_desc,extra_field_1,extra_field_2 A message,1331698658276340,2015-07-24T19:01:01+00:00,Write time,foo,bar ... Example JSONL file Unlike JSON files, imports in JSONL format can be streamed from disk, making them far less memory intensive than regular JSON files. {\"message\": \"A message\",\"timestamp\": 123456789,\"datetime\": \"2015-07-24T19:01:01+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"foo\"} {\"message\": \"Another message\",\"timestamp\": 123456790,\"datetime\": \"2015-07-24T19:01:02+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"bar\"} {\"message\": \"Yet more messages\",\"timestamp\": 123456791,\"datetime\": \"2015-07-24T19:01:03+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"baz\"} Upload the file to Timesketch To create a new timeline in Timesketch you need to upload it to the server. See here for instructions to do so","title":"Create timeline from JSON or CSV"},{"location":"learn/create-timeline-from-json-csv/#create-timeline-from-jsonl-or-csv-file","text":"You can ingest timeline data from a JSONL or CSV file. You can have any number of attributes/columns as you wish but there are some mandatory fields that Timeksketch needs in order to render the events in the UI. Mandatory fields: message String with an informative message of the event datetime ISO8601 format Ex: 2015-07-24T19:01:01+00:00 timestamp_desc String explaining what type of timestamp it is. E.g file created Ex: \"Time created\"","title":"Create timeline from JSONL or CSV file"},{"location":"learn/create-timeline-from-json-csv/#example-csv-file","text":"You need to provide the CSV header with the column names as the first line in the file. message,timestamp,datetime,timestamp_desc,extra_field_1,extra_field_2 A message,1331698658276340,2015-07-24T19:01:01+00:00,Write time,foo,bar ...","title":"Example CSV file"},{"location":"learn/create-timeline-from-json-csv/#example-jsonl-file","text":"Unlike JSON files, imports in JSONL format can be streamed from disk, making them far less memory intensive than regular JSON files. {\"message\": \"A message\",\"timestamp\": 123456789,\"datetime\": \"2015-07-24T19:01:01+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"foo\"} {\"message\": \"Another message\",\"timestamp\": 123456790,\"datetime\": \"2015-07-24T19:01:02+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"bar\"} {\"message\": \"Yet more messages\",\"timestamp\": 123456791,\"datetime\": \"2015-07-24T19:01:03+00:00\",\"timestamp_desc\": \"Write time\",\"extra_field_1\": \"baz\"}","title":"Example JSONL file"},{"location":"learn/create-timeline-from-json-csv/#upload-the-file-to-timesketch","text":"To create a new timeline in Timesketch you need to upload it to the server. See here for instructions to do so","title":"Upload the file to Timesketch"},{"location":"learn/notebook/","text":"Timesketch Notebook The Timesketch notebook is a docker container that runs a customized version of picatrix , designed to assist analysts using Timesketch. Installation The notebook is a docker container, so the first step is to make sure that docker is installed . If you did not install the docker desktop app you may also need to install docker-compose , please follow the instructions here (the version that is often included in your source repo might be too old to properly setup the container). After installing docker the next step is to create a docker compose file, which is used to bootstrap the docker commands. Save the following content to a file called docker-compose.yml : version: '3' services: notebook: container_name: notebook image: us-docker.pkg.dev/osdfir-registry/timesketch/notebook:latest ports: - 127.0.0.1:8844:8844 restart: on-failure volumes: - FOLDER_PATH:/usr/local/src/picadata/ Replace the text FOLDER_PATH with a folder that can survive reboots. This is the path to the folder where all notebooks will be saved to. The folder needs to be readable and writeable by a user with uid/gid 1000:1000 (if this is run on a Windows system the FOLDER_PATH can be set to something like C:\\My Folder\\ ) Once the file has been saved, docker-compose can be used to pull and start the container: $ sudo docker-compose pull $ sudo docker-compose up -d The docker compose command will download the latest build and deploy the TS docker container. Access the Container To be able to connect to the notebook connect to http://localhost:8844 , the password to access the notebook is timesketch . Troubleshooting Container In case there are any issues with the container it can be useful to take a look at the container logs, which may give you hints into what may be the issue. $ sudo docker container logs notebook TODO : Complete this section. Upgrade Container To update the container, use: $ sudo docker-compose pull $ sudo docker-compose stop $ sudo docker-compose up -d Docker Desktop If you are using Docker desktop you can find the docker image, click on the three dots and select pull. After manually updating the image the container needs to be recreated (using the docker compose up command used earlier). Credentials The docker container will have default credentials and configuration to connect to the developement server running in a container on the localhost, using the user/pass combination of dev/dev. To connect to a different server, few options are available: Copy ~/.timesketchrc and ~/.timesketch.token to the docker using docker cp . Run `ts_client = config.get_client(confirm_choices=True) and change all values as questions come up. Create a separate session using ts_client = config.get_client(config_section='myserver') The other option is to connect to the docker container: $ sudo docker exec -it notebook /bin/bash And manually craft the ~/.timesketchrc file. Connect To Colab In order to connect to the docker container from colab, select the arrow next to the Connect button, select Connect to local runtime and type in the URL http://localhost:8844/?token=timesketch into the Backend URL field and hit CONNECT . Usage TODO : This section needs to be filled in. However in the meantime these sites can be of an assistance: Discussion thread about the container Beginners guide of Jupyter Test notebook Jupyter tutorial","title":"Interactive notebook"},{"location":"learn/notebook/#timesketch-notebook","text":"The Timesketch notebook is a docker container that runs a customized version of picatrix , designed to assist analysts using Timesketch.","title":"Timesketch Notebook"},{"location":"learn/notebook/#installation","text":"The notebook is a docker container, so the first step is to make sure that docker is installed . If you did not install the docker desktop app you may also need to install docker-compose , please follow the instructions here (the version that is often included in your source repo might be too old to properly setup the container). After installing docker the next step is to create a docker compose file, which is used to bootstrap the docker commands. Save the following content to a file called docker-compose.yml : version: '3' services: notebook: container_name: notebook image: us-docker.pkg.dev/osdfir-registry/timesketch/notebook:latest ports: - 127.0.0.1:8844:8844 restart: on-failure volumes: - FOLDER_PATH:/usr/local/src/picadata/ Replace the text FOLDER_PATH with a folder that can survive reboots. This is the path to the folder where all notebooks will be saved to. The folder needs to be readable and writeable by a user with uid/gid 1000:1000 (if this is run on a Windows system the FOLDER_PATH can be set to something like C:\\My Folder\\ ) Once the file has been saved, docker-compose can be used to pull and start the container: $ sudo docker-compose pull $ sudo docker-compose up -d The docker compose command will download the latest build and deploy the TS docker container.","title":"Installation"},{"location":"learn/notebook/#access-the-container","text":"To be able to connect to the notebook connect to http://localhost:8844 , the password to access the notebook is timesketch .","title":"Access the Container"},{"location":"learn/notebook/#troubleshooting-container","text":"In case there are any issues with the container it can be useful to take a look at the container logs, which may give you hints into what may be the issue. $ sudo docker container logs notebook TODO : Complete this section.","title":"Troubleshooting Container"},{"location":"learn/notebook/#upgrade-container","text":"To update the container, use: $ sudo docker-compose pull $ sudo docker-compose stop $ sudo docker-compose up -d","title":"Upgrade Container"},{"location":"learn/notebook/#docker-desktop","text":"If you are using Docker desktop you can find the docker image, click on the three dots and select pull. After manually updating the image the container needs to be recreated (using the docker compose up command used earlier).","title":"Docker Desktop"},{"location":"learn/notebook/#credentials","text":"The docker container will have default credentials and configuration to connect to the developement server running in a container on the localhost, using the user/pass combination of dev/dev. To connect to a different server, few options are available: Copy ~/.timesketchrc and ~/.timesketch.token to the docker using docker cp . Run `ts_client = config.get_client(confirm_choices=True) and change all values as questions come up. Create a separate session using ts_client = config.get_client(config_section='myserver') The other option is to connect to the docker container: $ sudo docker exec -it notebook /bin/bash And manually craft the ~/.timesketchrc file.","title":"Credentials"},{"location":"learn/notebook/#connect-to-colab","text":"In order to connect to the docker container from colab, select the arrow next to the Connect button, select Connect to local runtime and type in the URL http://localhost:8844/?token=timesketch into the Backend URL field and hit CONNECT .","title":"Connect To Colab"},{"location":"learn/notebook/#usage","text":"TODO : This section needs to be filled in. However in the meantime these sites can be of an assistance: Discussion thread about the container Beginners guide of Jupyter Test notebook Jupyter tutorial","title":"Usage"},{"location":"learn/search-query-guide/","text":"Search within timeline Search queries Timesketch allows full text search within timelines. Good way to get started is by selecting one of pre-set search templates and adjusting them to the data in your timeline. Simple search queries relies on Query String Query mini-language, but it is also possible to use the full potential of Elasticsearch query language in Advanced queries. Common fields Data fields will vary depending on the source being uploaded, but here are some that are mandatory, and therefore will be present in any timeline. Field Description Example query message String with information about event message:\u201dThis is a message\u201d timestamp Timestamp as microseconds since Unix epoch timestamp:\u201d363420000\u201d datetime Date and time in ISO8601 format datetime:\u201d2016-03-31T22:56:32+00:00\u201d timestamp_desc String explaining what type of timestamp it is timestamp_desc:\u201dContent Modification Time\u201d Additional fields come from the imported Plaso file and depend on source type. You can see which additional fields are available in your timeline by clicking on any event and seeing the detailed list of all fields and their values. Field Description Example query data_type Data types present in timeline (depends on source) data_type:\"windows:registry:key_value\" filename Search for particular filetypes filename:*.exe strings: Search for a particular string strings:\"PsExec\" Search operators Query String supports boolean search operators AND, OR and NOT. Wildcards and regular expressions Wildcards can be run on individual search terms using ? for a single character and * for zero or more characters. Be aware that wildcards can use a lot of memory. Regular expression patterns can be embedded in the query string by wrapping them in forward-slashes (\"/\"): Syntax: Some characters are reserved for regular expressions and must be escaped in the pattern . ? + * | { } [ ] ( ) \" \\ Below are syntax elements and example regular expressions Sign Meaning Example \".\" Match any character For \"aaabbb\" : ab... # match a.c.e # match \"+\" One or more For \"aaabbb\" : a+b+ # match aa+bb+ # match a+.+ # match aa+bbb+ # match \"*\" Zero-or-more For \"aaabbb\" : a*b* # match a*b*c* # match .*bbb.* # match aaa*bbb* # match \"?\" Zero-or-one For \"aaabbb\" : aaa?bbb? # match aaaa?bbbb? # match .....?.? # match aa?bb? # no match \"{}\" Min-to-max repetitions For \"aaabbb\" : a{3}b{3} # match a{2,4}b{2,4} # match a{2,}b{2,} # match .{3}.{3} # match a{4}b{4} # no match a{4,6}b{4,6} # no match a{4,}b{4,} # no match \"()\" Forms sub-patterns For \"ababab\" (ab)+ # match ab(ab)+ # match ab(ab)+ # match (..)+ # match (...)+ # no match (ab)* # match abab(ab)? # match ab(ab)? # no match (ab){3} # match (ab){1,2} # no match \"|\" Acts as \"OR\" operator For \"aabb\" aabb|bbaa # match aacc|bb # no match aa(cc|bb) # match a+|b+ # no match a+b+|b+a+ # match a+(b|c)+ # match \"[]\" . Sets range of potential characters For \"abcd\": ab[cd]+ # match [a-d]+ # match [^a-d]+ # no match Advanced search Advanced search queries are in JSON format, and let you use the full power of Elasticsearch. You can view your existing Query String query as an advanced Elasticsearch query by clicking \"Advanced\" button below the query entry field. Full Elasticsearch guide Saved Searches Saved Searches are saved results of your search queries, for easier access later. A saved Search does not only include the query but also specifics like displayed columns. To save search results, run your search query, apply filters if needed, and click the \u201cSave\u201d button under the query field. Now you can access this Search from \u201cSaved Searches\u201d drop-down menu on Explore page of your sketch. You can further refine the data in your views by manually hiding certain events. To do it, click a small eye icon next to the icon. If you have hidden events in your view, they can be un-hidden by clicking red button \u201cShow hidden events\u201d in the upper right corner of your timeline. You can save changes to your views by clicking \u201cSave Changes\u201d button Search templates Search templates allow quick creation of most commonly used views. You can browse available templates in the \u201cSearch templates\u201d drop-down menu below search query window on \u201cExplore page\u201d On \u201cViews\u201d page, you can quickly generate and add a view from a template to your sketch. To do so, just scroll down to the template you want to use, and click \u201cQuick add\u201d","title":"Search query guide"},{"location":"learn/search-query-guide/#search-within-timeline","text":"","title":"Search within timeline"},{"location":"learn/search-query-guide/#search-queries","text":"Timesketch allows full text search within timelines. Good way to get started is by selecting one of pre-set search templates and adjusting them to the data in your timeline. Simple search queries relies on Query String Query mini-language, but it is also possible to use the full potential of Elasticsearch query language in Advanced queries.","title":"Search queries"},{"location":"learn/search-query-guide/#common-fields","text":"Data fields will vary depending on the source being uploaded, but here are some that are mandatory, and therefore will be present in any timeline. Field Description Example query message String with information about event message:\u201dThis is a message\u201d timestamp Timestamp as microseconds since Unix epoch timestamp:\u201d363420000\u201d datetime Date and time in ISO8601 format datetime:\u201d2016-03-31T22:56:32+00:00\u201d timestamp_desc String explaining what type of timestamp it is timestamp_desc:\u201dContent Modification Time\u201d Additional fields come from the imported Plaso file and depend on source type. You can see which additional fields are available in your timeline by clicking on any event and seeing the detailed list of all fields and their values. Field Description Example query data_type Data types present in timeline (depends on source) data_type:\"windows:registry:key_value\" filename Search for particular filetypes filename:*.exe strings: Search for a particular string strings:\"PsExec\"","title":"Common fields"},{"location":"learn/search-query-guide/#search-operators","text":"Query String supports boolean search operators AND, OR and NOT.","title":"Search operators"},{"location":"learn/search-query-guide/#wildcards-and-regular-expressions","text":"Wildcards can be run on individual search terms using ? for a single character and * for zero or more characters. Be aware that wildcards can use a lot of memory. Regular expression patterns can be embedded in the query string by wrapping them in forward-slashes (\"/\"):","title":"Wildcards and regular expressions"},{"location":"learn/search-query-guide/#syntax","text":"Some characters are reserved for regular expressions and must be escaped in the pattern . ? + * | { } [ ] ( ) \" \\ Below are syntax elements and example regular expressions Sign Meaning Example \".\" Match any character For \"aaabbb\" : ab... # match a.c.e # match \"+\" One or more For \"aaabbb\" : a+b+ # match aa+bb+ # match a+.+ # match aa+bbb+ # match \"*\" Zero-or-more For \"aaabbb\" : a*b* # match a*b*c* # match .*bbb.* # match aaa*bbb* # match \"?\" Zero-or-one For \"aaabbb\" : aaa?bbb? # match aaaa?bbbb? # match .....?.? # match aa?bb? # no match \"{}\" Min-to-max repetitions For \"aaabbb\" : a{3}b{3} # match a{2,4}b{2,4} # match a{2,}b{2,} # match .{3}.{3} # match a{4}b{4} # no match a{4,6}b{4,6} # no match a{4,}b{4,} # no match \"()\" Forms sub-patterns For \"ababab\" (ab)+ # match ab(ab)+ # match ab(ab)+ # match (..)+ # match (...)+ # no match (ab)* # match abab(ab)? # match ab(ab)? # no match (ab){3} # match (ab){1,2} # no match \"|\" Acts as \"OR\" operator For \"aabb\" aabb|bbaa # match aacc|bb # no match aa(cc|bb) # match a+|b+ # no match a+b+|b+a+ # match a+(b|c)+ # match \"[]\" . Sets range of potential characters For \"abcd\": ab[cd]+ # match [a-d]+ # match [^a-d]+ # no match","title":"Syntax:"},{"location":"learn/search-query-guide/#advanced-search","text":"Advanced search queries are in JSON format, and let you use the full power of Elasticsearch. You can view your existing Query String query as an advanced Elasticsearch query by clicking \"Advanced\" button below the query entry field. Full Elasticsearch guide","title":"Advanced search"},{"location":"learn/search-query-guide/#saved-searches","text":"Saved Searches are saved results of your search queries, for easier access later. A saved Search does not only include the query but also specifics like displayed columns. To save search results, run your search query, apply filters if needed, and click the \u201cSave\u201d button under the query field. Now you can access this Search from \u201cSaved Searches\u201d drop-down menu on Explore page of your sketch. You can further refine the data in your views by manually hiding certain events. To do it, click a small eye icon next to the icon. If you have hidden events in your view, they can be un-hidden by clicking red button \u201cShow hidden events\u201d in the upper right corner of your timeline. You can save changes to your views by clicking \u201cSave Changes\u201d button","title":"Saved Searches"},{"location":"learn/search-query-guide/#search-templates","text":"Search templates allow quick creation of most commonly used views. You can browse available templates in the \u201cSearch templates\u201d drop-down menu below search query window on \u201cExplore page\u201d On \u201cViews\u201d page, you can quickly generate and add a view from a template to your sketch. To do so, just scroll down to the template you want to use, and click \u201cQuick add\u201d","title":"Search templates"},{"location":"learn/server-admin/","text":"tsctl tsctl is a command line tool to control timesketch. Parameters: --config / -c (optional) Example tsctl runserver -c /etc/timesketch/timesketch.conf User management Adding users Command: tsctl add_user Parameters: --name / -n --password / -p (optional) Example tsctl add_user --name foo Change user password To change a user password, the add_user command can be used, as it is checking if the user exists if yes it will update the update. Command: tsctl add_user Parameters: --username / -u --password / -p (optional) Example tsctl add_user --username foo Removing users Not yet implemented. Group management Adding groups Command: tsctl add_group Parameters: --name / -n Removing groups Not yet implemented. Managing group membership Add or remove a user to a group. To add a user, specify the group and user. To remove a user, include the -r option. Command: tsctl manage_group Parameters: --remove / -r (optional) --group / -g --user / -u Example: tsctl manage_group -u user_foo -g group_bar add_index Create a new Timesketch searchindex. Command: tsctl add_index Parameters: --name / -n --index / -i --user / -u Example: tsctl add_index -u user_foo -i test_index_name -n sample Migrate db Command: tsctl db Drop database Will drop all databases. Command: tsctl drop_db Import json to Timesketch Command: tsctl json2ts Purge Delete timeline permanently from Timesketch and Elasticsearch. It will alert if a timeline is still in use in a sketch and prompt for confirmation before deletion. Args: index_name: The name of the index in Elasticsearch Command: tsctl purge search_template Export/Import search templates to/from file. Command: tsctl search_template Parameters: --import / -i --export / -e import_location: Path to the yaml file to import templates. export_location: Path to the yaml file to export templates. import Creates a new Timesketch timeline from a file. Supported file formats are: plaso, csv and jsonl. Command: tsctl import Parameters: --file / -f --sketch_id / -s (optional) --username / -f (optional) --timeline_name / -n (optional) The sketch id is inferred from the filename if it starts with a number. The timeline name can also be generated from the filename if not specified. similarity_score Command: tsctl similarity_score Upgrade DB After Schema Change After changing the schema for the database a revision file needs to be generated. (temporary solution) Before doing the database migration you'll need to modify the file timesketch/models/__init__.py : def init_db(): ... BaseModel.metadata.create_all(bind=engine) This line needs to be commented out, eg: def init_db(): ... #BaseModel.metadata.create_all(bind=engine) Then inside the timesketch container, to generate the file use the command: cd /usr/local/src/timesketch/timesketch tsctl db stamp head tsctl db upgrade This makes sure that the database is current. Then create a revision file: tsctl db migrate -m \"<message>\" Once the migration is done, remove the comment to re-enable the line in timesketch/models/__init.py . Troubleshooting Database Schema Changes If the migration file is not created, which could be an indication that the schema change is not detected by the automation one can create an empty revision file: tsctl db revision And then fill in the blanks, see examples of changes in timesketch/migrations/versions/*_.py .","title":"Server administration"},{"location":"learn/server-admin/#tsctl","text":"tsctl is a command line tool to control timesketch. Parameters: --config / -c (optional) Example tsctl runserver -c /etc/timesketch/timesketch.conf","title":"tsctl"},{"location":"learn/server-admin/#user-management","text":"","title":"User management"},{"location":"learn/server-admin/#adding-users","text":"Command: tsctl add_user Parameters: --name / -n --password / -p (optional) Example tsctl add_user --name foo","title":"Adding users"},{"location":"learn/server-admin/#change-user-password","text":"To change a user password, the add_user command can be used, as it is checking if the user exists if yes it will update the update. Command: tsctl add_user Parameters: --username / -u --password / -p (optional) Example tsctl add_user --username foo","title":"Change user password"},{"location":"learn/server-admin/#removing-users","text":"Not yet implemented.","title":"Removing users"},{"location":"learn/server-admin/#group-management","text":"","title":"Group management"},{"location":"learn/server-admin/#adding-groups","text":"Command: tsctl add_group Parameters: --name / -n","title":"Adding groups"},{"location":"learn/server-admin/#removing-groups","text":"Not yet implemented.","title":"Removing groups"},{"location":"learn/server-admin/#managing-group-membership","text":"Add or remove a user to a group. To add a user, specify the group and user. To remove a user, include the -r option. Command: tsctl manage_group Parameters: --remove / -r (optional) --group / -g --user / -u Example: tsctl manage_group -u user_foo -g group_bar","title":"Managing group membership"},{"location":"learn/server-admin/#add_index","text":"Create a new Timesketch searchindex. Command: tsctl add_index Parameters: --name / -n --index / -i --user / -u Example: tsctl add_index -u user_foo -i test_index_name -n sample","title":"add_index"},{"location":"learn/server-admin/#migrate-db","text":"Command: tsctl db","title":"Migrate db"},{"location":"learn/server-admin/#drop-database","text":"Will drop all databases. Command: tsctl drop_db","title":"Drop database"},{"location":"learn/server-admin/#import-json-to-timesketch","text":"Command: tsctl json2ts","title":"Import json to Timesketch"},{"location":"learn/server-admin/#purge","text":"Delete timeline permanently from Timesketch and Elasticsearch. It will alert if a timeline is still in use in a sketch and prompt for confirmation before deletion. Args: index_name: The name of the index in Elasticsearch Command: tsctl purge","title":"Purge"},{"location":"learn/server-admin/#search_template","text":"Export/Import search templates to/from file. Command: tsctl search_template Parameters: --import / -i --export / -e import_location: Path to the yaml file to import templates. export_location: Path to the yaml file to export templates.","title":"search_template"},{"location":"learn/server-admin/#import","text":"Creates a new Timesketch timeline from a file. Supported file formats are: plaso, csv and jsonl. Command: tsctl import Parameters: --file / -f --sketch_id / -s (optional) --username / -f (optional) --timeline_name / -n (optional) The sketch id is inferred from the filename if it starts with a number. The timeline name can also be generated from the filename if not specified.","title":"import"},{"location":"learn/server-admin/#similarity_score","text":"Command: tsctl similarity_score","title":"similarity_score"},{"location":"learn/server-admin/#upgrade-db-after-schema-change","text":"After changing the schema for the database a revision file needs to be generated. (temporary solution) Before doing the database migration you'll need to modify the file timesketch/models/__init__.py : def init_db(): ... BaseModel.metadata.create_all(bind=engine) This line needs to be commented out, eg: def init_db(): ... #BaseModel.metadata.create_all(bind=engine) Then inside the timesketch container, to generate the file use the command: cd /usr/local/src/timesketch/timesketch tsctl db stamp head tsctl db upgrade This makes sure that the database is current. Then create a revision file: tsctl db migrate -m \"<message>\" Once the migration is done, remove the comment to re-enable the line in timesketch/models/__init.py .","title":"Upgrade DB After Schema Change"},{"location":"learn/server-admin/#troubleshooting-database-schema-changes","text":"If the migration file is not created, which could be an indication that the schema change is not detected by the automation one can create an empty revision file: tsctl db revision And then fill in the blanks, see examples of changes in timesketch/migrations/versions/*_.py .","title":"Troubleshooting Database Schema Changes"},{"location":"learn/sigma/","text":"Sigma What is Sigma See description at the Sigma Github repository Sigma in Timesketch Since early 2020 Timesketch has Sigma support implemented. Sigma can be used as an analyser. The other option is to use Sigma via the API and the API client. Install rules Timesketch deliberately does not provide a set of Sigma rules, as those would add complexity to maintain. To use the official community rules you can clone github.com/Neo23x0/sigma to /data/sigma. This directory will not be caught by git. Warning: Currently it is not recommended to just clone the directory. See https://github.com/google/timesketch/issues/1532 for more info. cd data git clone https://github.com/Neo23x0/sigma The rules then will be under timesketch/data/sigma Sigma Rules The windows rules are stored in timesketch/data/sigma/rules/windows The linux rules are stored in timesketch/data/linux timesketch/data/sigma/rules/linux Sigma config In the config file sigma_config.yaml There is a section with mappings, most mappings where copied from HELK configuration. If you find a mapping missing, feel free to add and create a PR. Field Mapping Some adjustments verified: s/EventID/event_identifier s/Source/source_name Analyzer_run.py You can run the Sigma analyzer providing sample data: python3 test_tools/analyzer_run.py --test_file test_tools/test_events/sigma_events.jsonl timesketch/lib/analyzers/sigma_tagger.py RulesSigmaPlugin Test data If you want to test that feature, get some evtx files from the following links and parse it via plaso github.com/sbousseaden/EVTX-ATTACK-SAMPLES github.com/sans-blue-team/DeepBlueCLI/evtx Verify rules Deploying rules that can not be parsed by Sigma can cause problems on analyst side as well as Timesketch operator side. The analyst might not be able to see the logs and the errors might only occur when running the analyzer. This is why a standalone tool can be used from: test_tools/sigma_verify_rules.py This tool takes the following options: usage: sigma_verify_rules.py [-h] [--config_file PATH_TO_TEST_FILE] PATH_TO_RULES sigma_verify_rules.py: error: the following arguments are required: PATH_TO_RULES And could be used like the following to verify your rules would work: sigma_verify_rules.py --config_file ../data/sigma_config.yaml ../data/sigma/rules If any rules in that folder is causing problems it will be shown: sigma_verify_rules.py --config_file ../data/sigma_config.yaml ../timesketch/data/sigma/rules ERROR:root:reverse_shell.yaml Error generating rule in file ../timesketch/data/sigma/rules/linux/reverse_shell.yaml you should not use this rule in Timesketch: No condition found ERROR:root:recon_commands.yaml Error generating rule in file ../timesketch/data/sigma/rules/data/linux/recon_commands.yaml you should not use this rule in Timesketch: No condition found You should NOT import the following rules ../timesketch/data/sigma/rules/linux/reverse_shell.yaml ../timesketch/data/sigma/rules/linux/recon_commands.yaml Toubleshooting How to find issues Logs In the celery logs, while running the sigma analyzer, you will see something like that: result: Applied 0 tags * win_apt_carbonpaper_turla.yml: 0 ... * win_syskey_registry_access.yml: 0 Problematic rules: XXXX The XXX here is the \"problem\" and you should note those rules. Once you note and identified those rules, it is recommended to take the id and attempt a API call like the following: from timesketch_api_client import config ts = config.get_client() rule = ts.get_sigma_rule(\"c0478ead-5336-46c2-bd5e-b4c84bc3a36e\") print(rule.es_query) Where the ID is the id of your problematic rule. This will hopefully give you more insight from the web server logs of what caused the problem. E.g. \"Aggregations not implemented for this backend\" It is then recommended to move those rules to a separate folder, maybe even creating a small shell script that does that for you once you pull upstream rules from the Sigma repository. How to verify issues Timesketch API / logs If you have doubt if a rule does work, take the uuid and run python code mentioned above. sigmac Another option is to run the rule against the official sigma client with the Timesketch sigma mapping file. For our example from above: sigma/tools/sigma$ python3 sigmac.py -t es-qs --config ../../../sigma_config.yaml ../../rules/windows/image_load/sysmon_mimikatz_inmemory_detection.yml An unsupported feature is required for this Sigma rule (../../rules/windows/image_load/sysmon_mimikatz_inmemory_detection.yml): Aggregations not implemented for this backend Feel free to contribute for fun and fame, this is open source :) -> https://github.com/Neo23x0/sigma What to do with problematic rules To reduce load on the system it is recommended to not keep the problematic rules in the directory, as it will cause the exception every time the rules folders are parsed (a lot!). The parser is made to ignore \"deprecated\" folders. So you could move the problematic rules to your rules folder in a subfolder /deprecated/. If the rules do not contain any sensitive content, you could also open an issue in the timesketch project and or in the upstream sigma project and explain your issue (best case: provide your timesketch sigma config and the rule file so it can be verified).","title":"Use Sigma"},{"location":"learn/sigma/#sigma","text":"","title":"Sigma"},{"location":"learn/sigma/#what-is-sigma","text":"See description at the Sigma Github repository","title":"What is Sigma"},{"location":"learn/sigma/#sigma-in-timesketch","text":"Since early 2020 Timesketch has Sigma support implemented. Sigma can be used as an analyser. The other option is to use Sigma via the API and the API client.","title":"Sigma in Timesketch"},{"location":"learn/sigma/#install-rules","text":"Timesketch deliberately does not provide a set of Sigma rules, as those would add complexity to maintain. To use the official community rules you can clone github.com/Neo23x0/sigma to /data/sigma. This directory will not be caught by git. Warning: Currently it is not recommended to just clone the directory. See https://github.com/google/timesketch/issues/1532 for more info. cd data git clone https://github.com/Neo23x0/sigma The rules then will be under timesketch/data/sigma","title":"Install rules"},{"location":"learn/sigma/#sigma-rules","text":"The windows rules are stored in timesketch/data/sigma/rules/windows The linux rules are stored in timesketch/data/linux timesketch/data/sigma/rules/linux","title":"Sigma Rules"},{"location":"learn/sigma/#sigma-config","text":"In the config file sigma_config.yaml There is a section with mappings, most mappings where copied from HELK configuration. If you find a mapping missing, feel free to add and create a PR.","title":"Sigma config"},{"location":"learn/sigma/#field-mapping","text":"Some adjustments verified: s/EventID/event_identifier s/Source/source_name","title":"Field Mapping"},{"location":"learn/sigma/#analyzer_runpy","text":"You can run the Sigma analyzer providing sample data: python3 test_tools/analyzer_run.py --test_file test_tools/test_events/sigma_events.jsonl timesketch/lib/analyzers/sigma_tagger.py RulesSigmaPlugin","title":"Analyzer_run.py"},{"location":"learn/sigma/#test-data","text":"If you want to test that feature, get some evtx files from the following links and parse it via plaso github.com/sbousseaden/EVTX-ATTACK-SAMPLES github.com/sans-blue-team/DeepBlueCLI/evtx","title":"Test data"},{"location":"learn/sigma/#verify-rules","text":"Deploying rules that can not be parsed by Sigma can cause problems on analyst side as well as Timesketch operator side. The analyst might not be able to see the logs and the errors might only occur when running the analyzer. This is why a standalone tool can be used from: test_tools/sigma_verify_rules.py This tool takes the following options: usage: sigma_verify_rules.py [-h] [--config_file PATH_TO_TEST_FILE] PATH_TO_RULES sigma_verify_rules.py: error: the following arguments are required: PATH_TO_RULES And could be used like the following to verify your rules would work: sigma_verify_rules.py --config_file ../data/sigma_config.yaml ../data/sigma/rules If any rules in that folder is causing problems it will be shown: sigma_verify_rules.py --config_file ../data/sigma_config.yaml ../timesketch/data/sigma/rules ERROR:root:reverse_shell.yaml Error generating rule in file ../timesketch/data/sigma/rules/linux/reverse_shell.yaml you should not use this rule in Timesketch: No condition found ERROR:root:recon_commands.yaml Error generating rule in file ../timesketch/data/sigma/rules/data/linux/recon_commands.yaml you should not use this rule in Timesketch: No condition found You should NOT import the following rules ../timesketch/data/sigma/rules/linux/reverse_shell.yaml ../timesketch/data/sigma/rules/linux/recon_commands.yaml","title":"Verify rules"},{"location":"learn/sigma/#toubleshooting","text":"","title":"Toubleshooting"},{"location":"learn/sigma/#how-to-find-issues","text":"","title":"How to find issues"},{"location":"learn/sigma/#logs","text":"In the celery logs, while running the sigma analyzer, you will see something like that: result: Applied 0 tags * win_apt_carbonpaper_turla.yml: 0 ... * win_syskey_registry_access.yml: 0 Problematic rules: XXXX The XXX here is the \"problem\" and you should note those rules. Once you note and identified those rules, it is recommended to take the id and attempt a API call like the following: from timesketch_api_client import config ts = config.get_client() rule = ts.get_sigma_rule(\"c0478ead-5336-46c2-bd5e-b4c84bc3a36e\") print(rule.es_query) Where the ID is the id of your problematic rule. This will hopefully give you more insight from the web server logs of what caused the problem. E.g. \"Aggregations not implemented for this backend\" It is then recommended to move those rules to a separate folder, maybe even creating a small shell script that does that for you once you pull upstream rules from the Sigma repository.","title":"Logs"},{"location":"learn/sigma/#how-to-verify-issues","text":"","title":"How to verify issues"},{"location":"learn/sigma/#timesketch-api-logs","text":"If you have doubt if a rule does work, take the uuid and run python code mentioned above.","title":"Timesketch API / logs"},{"location":"learn/sigma/#sigmac","text":"Another option is to run the rule against the official sigma client with the Timesketch sigma mapping file. For our example from above: sigma/tools/sigma$ python3 sigmac.py -t es-qs --config ../../../sigma_config.yaml ../../rules/windows/image_load/sysmon_mimikatz_inmemory_detection.yml An unsupported feature is required for this Sigma rule (../../rules/windows/image_load/sysmon_mimikatz_inmemory_detection.yml): Aggregations not implemented for this backend Feel free to contribute for fun and fame, this is open source :) -> https://github.com/Neo23x0/sigma","title":"sigmac"},{"location":"learn/sigma/#what-to-do-with-problematic-rules","text":"To reduce load on the system it is recommended to not keep the problematic rules in the directory, as it will cause the exception every time the rules folders are parsed (a lot!). The parser is made to ignore \"deprecated\" folders. So you could move the problematic rules to your rules folder in a subfolder /deprecated/. If the rules do not contain any sensitive content, you could also open an issue in the timesketch project and or in the upstream sigma project and explain your issue (best case: provide your timesketch sigma config and the rule file so it can be verified).","title":"What to do with problematic rules"},{"location":"learn/sketch-overview/","text":"Sketches Sketches are a way to organise analysis of events across multiple timelines and increase data discoverability via search, targeted views, comments and stories. Creating a sketch To create a new sketch, click \u201cNew Sketch\u201d button on the tool\u2019s homepage. After that you will be redirected straight onto Overview page of your new sketch. Here you can click \u201cEdit\u201d icon to give your sketch a name and enter a description. Now you need to add timelines to your new sketch. To do so, you can click \u201cImport Timeline\u201d and upload a file in Plaso, CSV or JSONL formats. More details about importing timelines are in chapter \u201cImporting timelines\u201d Alternatively, you can go to \u201cTimelines\u201d tab and select from all available timelines. Navigating a sketch A sketch consists of 5 tabs: \u201cOverview\u201d, \u201cExplore\u201d, \u201cStories\u201d Overview tab contains summary information about your sketch, such as sketch title, description, as well as shortcuts to saved views and timelines. Explore page allows navigating timelines, using search queries, applying filters, viewing timeline data in chart format and saving your search discoveries as new views. Stories tab allows creating outlines of your Stories can be annotated with selected items from your timelines Sharing and access control After the sketch is created, you can share it with other users in the system. To do so, click button. You will be presented with the following dialogue: You can share the sketch with users, groups of users, make it available to all users in your system, or leave private. Explore Search See Search query guide Views allows quick access to saved views and creation of new views Timerange allows to control the timerange of shown events. Filter Add a filter, e.g. to show only starred events More If you click on the \"More\" Button in the Sketch Overview, you get the following three options. Delete Delete the whole sketch. Note: this will not delete the Timelines. Archive Archive the whole sketch. Export Export will export the following items: events (starred, tagged, tagged_event_stats, comments, ...) stories as html views (as csv)","title":"Sketch overview"},{"location":"learn/sketch-overview/#sketches","text":"Sketches are a way to organise analysis of events across multiple timelines and increase data discoverability via search, targeted views, comments and stories.","title":"Sketches"},{"location":"learn/sketch-overview/#creating-a-sketch","text":"To create a new sketch, click \u201cNew Sketch\u201d button on the tool\u2019s homepage. After that you will be redirected straight onto Overview page of your new sketch. Here you can click \u201cEdit\u201d icon to give your sketch a name and enter a description. Now you need to add timelines to your new sketch. To do so, you can click \u201cImport Timeline\u201d and upload a file in Plaso, CSV or JSONL formats. More details about importing timelines are in chapter \u201cImporting timelines\u201d Alternatively, you can go to \u201cTimelines\u201d tab and select from all available timelines.","title":"Creating a sketch"},{"location":"learn/sketch-overview/#navigating-a-sketch","text":"A sketch consists of 5 tabs: \u201cOverview\u201d, \u201cExplore\u201d, \u201cStories\u201d Overview tab contains summary information about your sketch, such as sketch title, description, as well as shortcuts to saved views and timelines. Explore page allows navigating timelines, using search queries, applying filters, viewing timeline data in chart format and saving your search discoveries as new views. Stories tab allows creating outlines of your Stories can be annotated with selected items from your timelines","title":"Navigating a sketch"},{"location":"learn/sketch-overview/#sharing-and-access-control","text":"After the sketch is created, you can share it with other users in the system. To do so, click button. You will be presented with the following dialogue: You can share the sketch with users, groups of users, make it available to all users in your system, or leave private.","title":"Sharing and access control"},{"location":"learn/sketch-overview/#explore","text":"Search See Search query guide Views allows quick access to saved views and creation of new views Timerange allows to control the timerange of shown events. Filter Add a filter, e.g. to show only starred events","title":"Explore"},{"location":"learn/sketch-overview/#more","text":"If you click on the \"More\" Button in the Sketch Overview, you get the following three options.","title":"More"},{"location":"learn/sketch-overview/#delete","text":"Delete the whole sketch. Note: this will not delete the Timelines.","title":"Delete"},{"location":"learn/sketch-overview/#archive","text":"Archive the whole sketch.","title":"Archive"},{"location":"learn/sketch-overview/#export","text":"Export will export the following items: events (starred, tagged, tagged_event_stats, comments, ...) stories as html views (as csv)","title":"Export"},{"location":"learn/upgrade/","text":"Upgrade an existing installation When upgrading Timesketch you might need to migrate the database to use the latest database schema. This is how you do that. Backup you database (!) First you should backup your current database in case something goes wrong in the upgrade process. For PostgreSQL you do the following (Ref: https://www.postgresql.org/docs/9.1/static/backup.html): $ sudo -u postgres pg_dump timesketch > ~/timesketch-db.sql $ sudo -u postgres pg_dumpall > ~/timesketch-db-all.sql Change to your Timesketch installation directory (e.g. /opt/timesketch) $ cd /<PATH TO TIMESKETCH INSTALLATION> Upgrade the database schema Have you backed up your database..? good. Let's upgrade the schema. First connect to the timesketch-web container: $ docker-compose exec timesketch-web /bin/bash While connected to the container: root@<CONTAINER_ID>$ git clone https://github.com/google/timesketch.git root@<CONTAINER_ID>$ cd timesketch/timesketch root@<CONTAINER_ID>$ tsctl db current If this command returns a value, then you can go ahead and upgrade the database. In case you don't get any response back from the db current command you'll need to first find out revisions and fix a spot to upgrade from. root@<CONTAINER_ID>$ tsctl db history Find the lasat revision number you have upgraded the database too, and then issue root@<CONTAINER_ID>$ tsctl db stamp <REVISION_ID> And now you are ready to upgrade. root@<CONTAINER_ID>$ tsctl db upgrade Upgrade timesketch Exit from the container (CTRL-D), then pull new versions of the docker images and upgrade Timesketch: $ docker-compose pull $ docker-compose down $ docker-compose up -d","title":"Upgrade"},{"location":"learn/upgrade/#upgrade-an-existing-installation","text":"When upgrading Timesketch you might need to migrate the database to use the latest database schema. This is how you do that.","title":"Upgrade an existing installation"},{"location":"learn/upgrade/#backup-you-database","text":"First you should backup your current database in case something goes wrong in the upgrade process. For PostgreSQL you do the following (Ref: https://www.postgresql.org/docs/9.1/static/backup.html): $ sudo -u postgres pg_dump timesketch > ~/timesketch-db.sql $ sudo -u postgres pg_dumpall > ~/timesketch-db-all.sql","title":"Backup you database (!)"},{"location":"learn/upgrade/#change-to-your-timesketch-installation-directory","text":"(e.g. /opt/timesketch) $ cd /<PATH TO TIMESKETCH INSTALLATION>","title":"Change to your Timesketch installation directory"},{"location":"learn/upgrade/#upgrade-the-database-schema","text":"Have you backed up your database..? good. Let's upgrade the schema. First connect to the timesketch-web container: $ docker-compose exec timesketch-web /bin/bash While connected to the container: root@<CONTAINER_ID>$ git clone https://github.com/google/timesketch.git root@<CONTAINER_ID>$ cd timesketch/timesketch root@<CONTAINER_ID>$ tsctl db current If this command returns a value, then you can go ahead and upgrade the database. In case you don't get any response back from the db current command you'll need to first find out revisions and fix a spot to upgrade from. root@<CONTAINER_ID>$ tsctl db history Find the lasat revision number you have upgraded the database too, and then issue root@<CONTAINER_ID>$ tsctl db stamp <REVISION_ID> And now you are ready to upgrade. root@<CONTAINER_ID>$ tsctl db upgrade","title":"Upgrade the database schema"},{"location":"learn/upgrade/#upgrade-timesketch","text":"Exit from the container (CTRL-D), then pull new versions of the docker images and upgrade Timesketch: $ docker-compose pull $ docker-compose down $ docker-compose up -d","title":"Upgrade timesketch"}]}